{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_nb.ipynb","provenance":[],"authorship_tag":"ABX9TyN/oIAupLFVPEWLOg7peMwB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pdn6o94aonF6","executionInfo":{"status":"ok","timestamp":1620562457862,"user_tz":240,"elapsed":21752,"user":{"displayName":"Aathira Manoj","photoUrl":"","userId":"16371146117290387708"}},"outputId":"187df652-101d-4ed0-afe9-7876057d4c62"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd  /content/drive/'My Drive'/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iMVju--foyYM","executionInfo":{"status":"ok","timestamp":1620562461338,"user_tz":240,"elapsed":779,"user":{"displayName":"Aathira Manoj","photoUrl":"","userId":"16371146117290387708"}},"outputId":"350feebc-e5fd-4d63-ba48-ca6558131642"},"source":["!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["action_gen_real.txt  data.py\t       model_output\t reward_gen_real.txt\n","action_gen.txt\t     discriminator.py  nn_layer.py\t reward_gen.txt\n","agent.py\t     eval.py\t       pg_accuracy6.png  tar_gen_real.txt\n","click_gen_real.txt   generator.py      pg_map6.png\t tar_gen.txt\n","click_gen.txt\t     helper.py\t       __pycache__\t train.py\n","data.png\t     main.py\t       replay.py\t util.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nR0GODvXo7-A","executionInfo":{"status":"ok","timestamp":1620565913551,"user_tz":240,"elapsed":3450297,"user":{"displayName":"Aathira Manoj","photoUrl":"","userId":"16371146117290387708"}},"outputId":"e6edb5dd-38e4-4044-fb71-e9962499e8f1"},"source":["!python main.py --click ../simulation_task1/gen_click.txt --reward ../simulation_task1/gen_reward.txt --action ../simulation_task1/gen_action.txt --model LSTM --nhid 128 --n_layers_usr 2 --optim_nll adam --optim_adv adam --batch_size 128"],"execution_count":3,"outputs":[{"output_type":"stream","text":["==============================================\n","Training on the epoch:0\n","==============================================\n","100\n","Train seq : 8000\n","Valid seq : 1000\n","Test seq : 1000\n","20\n","The generator is:\n","Generator(\n","  (embedding): EmbeddingLayer(\n","    (drop): Dropout(p=0, inplace=False)\n","    (embedding): Embedding(100, 50)\n","  )\n","  (encoder): Encoder(\n","    (enc_lstm): LSTM(50, 128, num_layers=2, batch_first=True)\n","  )\n","  (enc2out): Linear(in_features=128, out_features=50, bias=True)\n","  (enc2rewd): Linear(in_features=128, out_features=50, bias=True)\n",")\n","The agent is:\n","Agent(\n","  (embedding): EmbeddingLayer(\n","    (drop): Dropout(p=0.25, inplace=False)\n","    (embedding): Embedding(99, 50)\n","  )\n","  (encoder): Encoder(\n","    (enc_lstm): LSTM(50, 50, batch_first=True)\n","  )\n","  (enc2out): Linear(in_features=50, out_features=99, bias=True)\n",")\n","The discriminator is:\n","Discriminator(\n","  (embedding): EmbeddingLayer(\n","    (drop): Dropout(p=0.25, inplace=False)\n","    (embedding): Embedding(100, 50)\n","  )\n","  (encoder): Encoder(\n","    (enc_lstm): LSTM(71, 128, batch_first=True)\n","  )\n","  (enc2out): Linear(in_features=128, out_features=2, bias=True)\n","  (rec2enc): Linear(in_features=1000, out_features=20, bias=True)\n",")\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:215: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  trainSample.genSample_pred(np.array(clicklist)[trainindex].tolist(), np.array(rewardlist)[trainindex].tolist(), np.array(actionlist)[trainindex].tolist(), warm_up, real_num_label, rec_len, add_end = True)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:216: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  validSample.genSample_predtest(np.array(clicklist)[validindex].tolist(),np.array(rewardlist)[validindex].tolist(), np.array(actionlist)[validindex].tolist(), warm_up, real_num_label, add_end = False)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:217: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  testSample.genSample_predtest(np.array(clicklist)[testindex].tolist(),np.array(rewardlist)[testindex].tolist(), np.array(actionlist)[testindex].tolist(), warm_up, real_num_label, add_end = False)\n","\n","--------------------------------------------\n","Pretrain Generator with given recommendation\n","--------------------------------------------\n","\n","GENERATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:153: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return np.array(self.itemSeq[batchstart:batchend]), np.array(self.target[batchstart:batchend]), np.array(self.reward[batchstart:batchend]), np.array(self.action[batchstart:batchend])\n","results : epoch 1 ; mean accuracy pred : 23.36; mean P@10 pred: 64.23; mean accuracy reward: 64.65\n","User model evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 10.92, precision@10 valid : 61.86, reward_accuracy valid 57.33\n","saving model at epoch 1\n","\n","GENERATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","results : epoch 2 ; mean accuracy pred : 26.77; mean P@10 pred: 68.4; mean accuracy reward: 67.01\n","User model evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 13.14, precision@10 valid : 64.11, reward_accuracy valid 57.05\n","saving model at epoch 2\n","\n","GENERATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","results : epoch 3 ; mean accuracy pred : 27.48; mean P@10 pred: 69.91; mean accuracy reward: 67.54\n","User model evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 13.14, precision@10 valid : 66.35, reward_accuracy valid 57.83\n","saving model at epoch 3\n","\n","GENERATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","results : epoch 4 ; mean accuracy pred : 27.66; mean P@10 pred: 72.08; mean accuracy reward: 67.74\n","User model evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 13.48, precision@10 valid : 67.82, reward_accuracy valid 58.4\n","saving model at epoch 4\n","\n","GENERATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","results : epoch 5 ; mean accuracy pred : 27.76; mean P@10 pred: 73.33; mean accuracy reward: 67.86\n","User model evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 13.76, precision@10 valid : 68.6, reward_accuracy valid 58.49\n","saving model at epoch 5\n","\n","GENERATOR TRAINING : Epoch 6\n","Learning rate : 0.0007737809374999998\n","results : epoch 6 ; mean accuracy pred : 27.88; mean P@10 pred: 74.06; mean accuracy reward: 67.9\n","User model evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 13.86, precision@10 valid : 69.35, reward_accuracy valid 58.86\n","saving model at epoch 6\n","\n","GENERATOR TRAINING : Epoch 7\n","Learning rate : 0.0007350918906249997\n","results : epoch 7 ; mean accuracy pred : 27.9; mean P@10 pred: 74.27; mean accuracy reward: 67.92\n","User model evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 14.2, precision@10 valid : 69.88, reward_accuracy valid 58.68\n","saving model at epoch 7\n","\n","GENERATOR TRAINING : Epoch 8\n","Learning rate : 0.0006983372960937497\n","results : epoch 8 ; mean accuracy pred : 28.08; mean P@10 pred: 74.29; mean accuracy reward: 67.96\n","User model evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 14.95, precision@10 valid : 70.04, reward_accuracy valid 58.52\n","saving model at epoch 8\n","\n","GENERATOR TRAINING : Epoch 9\n","Learning rate : 0.0006634204312890621\n","results : epoch 9 ; mean accuracy pred : 28.34; mean P@10 pred: 74.37; mean accuracy reward: 68.0\n","User model evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 15.04, precision@10 valid : 70.13, reward_accuracy valid 58.55\n","saving model at epoch 9\n","\n","GENERATOR TRAINING : Epoch 10\n","Learning rate : 0.000630249409724609\n","results : epoch 10 ; mean accuracy pred : 28.45; mean P@10 pred: 74.44; mean accuracy reward: 67.99\n","User model evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 14.73, precision@10 valid : 70.22, reward_accuracy valid 58.46\n","saving model at epoch 10\n","\n","GENERATOR TRAINING : Epoch 11\n","Learning rate : 0.0005987369392383785\n","results : epoch 11 ; mean accuracy pred : 28.5; mean P@10 pred: 74.49; mean accuracy reward: 67.99\n","User model evaluation!\n","togrep : results : epoch 11 ; accuracy valid : 14.79, precision@10 valid : 70.57, reward_accuracy valid 58.49\n","saving model at epoch 11\n","\n","GENERATOR TRAINING : Epoch 12\n","Learning rate : 0.0005688000922764595\n","results : epoch 12 ; mean accuracy pred : 28.49; mean P@10 pred: 74.67; mean accuracy reward: 67.98\n","User model evaluation!\n","togrep : results : epoch 12 ; accuracy valid : 14.83, precision@10 valid : 70.66, reward_accuracy valid 58.43\n","saving model at epoch 12\n","\n","GENERATOR TRAINING : Epoch 13\n","Learning rate : 0.0005403600876626365\n","results : epoch 13 ; mean accuracy pred : 28.5; mean P@10 pred: 74.73; mean accuracy reward: 67.99\n","User model evaluation!\n","togrep : results : epoch 13 ; accuracy valid : 14.86, precision@10 valid : 70.91, reward_accuracy valid 58.49\n","saving model at epoch 13\n","\n","GENERATOR TRAINING : Epoch 14\n","Learning rate : 0.0005133420832795047\n","results : epoch 14 ; mean accuracy pred : 28.5; mean P@10 pred: 75.02; mean accuracy reward: 67.97\n","User model evaluation!\n","togrep : results : epoch 14 ; accuracy valid : 14.89, precision@10 valid : 70.94, reward_accuracy valid 58.55\n","saving model at epoch 14\n","\n","GENERATOR TRAINING : Epoch 15\n","Learning rate : 0.00048767497911552944\n","results : epoch 15 ; mean accuracy pred : 28.49; mean P@10 pred: 75.11; mean accuracy reward: 68.01\n","User model evaluation!\n","togrep : results : epoch 15 ; accuracy valid : 14.92, precision@10 valid : 71.44, reward_accuracy valid 58.55\n","saving model at epoch 15\n","\n","GENERATOR TRAINING : Epoch 16\n","Learning rate : 0.00046329123015975297\n","results : epoch 16 ; mean accuracy pred : 28.48; mean P@10 pred: 75.2; mean accuracy reward: 68.02\n","User model evaluation!\n","togrep : results : epoch 16 ; accuracy valid : 14.89, precision@10 valid : 71.57, reward_accuracy valid 58.65\n","saving model at epoch 16\n","\n","GENERATOR TRAINING : Epoch 17\n","Learning rate : 0.0004401266686517653\n","results : epoch 17 ; mean accuracy pred : 28.51; mean P@10 pred: 75.28; mean accuracy reward: 68.02\n","User model evaluation!\n","togrep : results : epoch 17 ; accuracy valid : 15.01, precision@10 valid : 71.6, reward_accuracy valid 58.65\n","saving model at epoch 17\n","\n","GENERATOR TRAINING : Epoch 18\n","Learning rate : 0.00041812033521917703\n","results : epoch 18 ; mean accuracy pred : 28.53; mean P@10 pred: 75.33; mean accuracy reward: 68.02\n","User model evaluation!\n","togrep : results : epoch 18 ; accuracy valid : 15.04, precision@10 valid : 71.72, reward_accuracy valid 58.65\n","saving model at epoch 18\n","\n","GENERATOR TRAINING : Epoch 19\n","Learning rate : 0.00039721431845821814\n","results : epoch 19 ; mean accuracy pred : 28.53; mean P@10 pred: 75.53; mean accuracy reward: 68.01\n","User model evaluation!\n","togrep : results : epoch 19 ; accuracy valid : 15.04, precision@10 valid : 72.0, reward_accuracy valid 58.65\n","saving model at epoch 19\n","\n","GENERATOR TRAINING : Epoch 20\n","Learning rate : 0.0003773536025353072\n","results : epoch 20 ; mean accuracy pred : 28.55; mean P@10 pred: 75.62; mean accuracy reward: 68.03\n","User model evaluation!\n","togrep : results : epoch 20 ; accuracy valid : 15.01, precision@10 valid : 72.28, reward_accuracy valid 58.65\n","saving model at epoch 20\n","\n","GENERATOR TRAINING : Epoch 21\n","Learning rate : 0.0003584859224085418\n","results : epoch 21 ; mean accuracy pred : 28.56; mean P@10 pred: 75.67; mean accuracy reward: 68.03\n","User model evaluation!\n","togrep : results : epoch 21 ; accuracy valid : 15.01, precision@10 valid : 72.28, reward_accuracy valid 58.65\n","\n","GENERATOR TRAINING : Epoch 22\n","Learning rate : 0.0003405616262881147\n","results : epoch 22 ; mean accuracy pred : 28.56; mean P@10 pred: 75.8; mean accuracy reward: 68.06\n","User model evaluation!\n","togrep : results : epoch 22 ; accuracy valid : 15.01, precision@10 valid : 72.47, reward_accuracy valid 58.49\n","saving model at epoch 22\n","\n","GENERATOR TRAINING : Epoch 23\n","Learning rate : 0.00032353354497370894\n","results : epoch 23 ; mean accuracy pred : 28.58; mean P@10 pred: 75.86; mean accuracy reward: 68.06\n","User model evaluation!\n","togrep : results : epoch 23 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","saving model at epoch 23\n","\n","GENERATOR TRAINING : Epoch 24\n","Learning rate : 0.00030735686772502346\n","results : epoch 24 ; mean accuracy pred : 28.59; mean P@10 pred: 75.88; mean accuracy reward: 68.06\n","User model evaluation!\n","togrep : results : epoch 24 ; accuracy valid : 15.11, precision@10 valid : 72.63, reward_accuracy valid 58.52\n","\n","GENERATOR TRAINING : Epoch 25\n","Learning rate : 0.00029198902433877225\n","results : epoch 25 ; mean accuracy pred : 28.57; mean P@10 pred: 75.88; mean accuracy reward: 68.06\n","User model evaluation!\n","togrep : results : epoch 25 ; accuracy valid : 15.11, precision@10 valid : 72.66, reward_accuracy valid 58.52\n","Testing\n","User model evaluation!\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; accuracy test : 14.61, precision@10 test : 70.97, reward_accuracy test 58.27\n","\n","--------------------------------------------\n","Pretrain by the adversarial training\n","---------------------------------------------\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:220: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  trainSample.genSample_generator(np.array(clicklist)[trainindex].tolist(), np.array(rewardlist)[trainindex].tolist(), np.array(actionlist)[trainindex].tolist(), real_num_label, rec_len, add_end = True)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:221: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  validSample.genSample_predtest(np.array(clicklist)[validindex].tolist(),np.array(rewardlist)[validindex].tolist(), np.array(actionlist)[validindex].tolist(), warm_up, real_num_label, add_end = False)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:222: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  testSample.genSample_predtest(np.array(clicklist)[testindex].tolist(),np.array(rewardlist)[testindex].tolist(), np.array(actionlist)[testindex].tolist(), warm_up, real_num_label, add_end = False)\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 1\n","Learning rate_agent : 0.001\n","Learning rate_usr : 0.001\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:158: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  self.itemSeq = np.array(origin.itemSeq)[index[:subnum]].tolist()\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  self.reward = np.array(origin.reward)[index[:subnum]].tolist()\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  self.action = np.array(origin.action)[index[:subnum]].tolist()\n","Agent evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 9.52, precision@10 valid : 64.83\n","User model evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 1 ; mean accuracy pred valid : 3.4, map pred valid : 9.85; mean accuracy reward valid : 58.52\n","saving model at epoch 1\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 2\n","Learning rate_agent : 0.00095\n","Learning rate_usr : 0.00095\n","Agent evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 9.8, precision@10 valid : 68.82\n","User model evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 2 ; mean accuracy pred valid : 3.4, map pred valid : 11.16; mean accuracy reward valid : 58.52\n","saving model at epoch 2\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 3\n","Learning rate_agent : 0.0009025\n","Learning rate_usr : 0.0009025\n","Agent evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 9.86, precision@10 valid : 70.22\n","User model evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 3 ; mean accuracy pred valid : 3.4, map pred valid : 11.19; mean accuracy reward valid : 58.52\n","saving model at epoch 3\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 4\n","Learning rate_agent : 0.000857375\n","Learning rate_usr : 0.000857375\n","Agent evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 10.14, precision@10 valid : 71.16\n","User model evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 4 ; mean accuracy pred valid : 3.4, map pred valid : 11.28; mean accuracy reward valid : 58.52\n","saving model at epoch 4\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 5\n","Learning rate_agent : 0.0008145062499999999\n","Learning rate_usr : 0.0008145062499999999\n","Agent evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 10.14, precision@10 valid : 71.88\n","User model evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 5 ; mean accuracy pred valid : 3.4, map pred valid : 11.32; mean accuracy reward valid : 58.52\n","saving model at epoch 5\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 6\n","Learning rate_agent : 0.0007737809374999998\n","Learning rate_usr : 0.0007737809374999998\n","Agent evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 10.42, precision@10 valid : 72.22\n","User model evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 6 ; mean accuracy pred valid : 3.4, map pred valid : 11.35; mean accuracy reward valid : 58.52\n","saving model at epoch 6\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 7\n","Learning rate_agent : 0.0007350918906249997\n","Learning rate_usr : 0.0007350918906249997\n","Agent evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 10.83, precision@10 valid : 72.85\n","User model evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 7 ; mean accuracy pred valid : 3.43, map pred valid : 11.39; mean accuracy reward valid : 58.52\n","saving model at epoch 7\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 8\n","Learning rate_agent : 0.0006983372960937497\n","Learning rate_usr : 0.0006983372960937497\n","Agent evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 10.96, precision@10 valid : 72.94\n","User model evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 8 ; mean accuracy pred valid : 3.43, map pred valid : 11.44; mean accuracy reward valid : 58.52\n","saving model at epoch 8\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 9\n","Learning rate_agent : 0.0006634204312890621\n","Learning rate_usr : 0.0006634204312890621\n","Agent evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 10.99, precision@10 valid : 73.16\n","User model evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 9 ; mean accuracy pred valid : 3.43, map pred valid : 11.45; mean accuracy reward valid : 58.52\n","saving model at epoch 9\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 10\n","Learning rate_agent : 0.000630249409724609\n","Learning rate_usr : 0.000630249409724609\n","Agent evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 11.24, precision@10 valid : 72.94\n","User model evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 10 ; mean accuracy pred valid : 3.46, map pred valid : 11.48; mean accuracy reward valid : 58.52\n","saving model at epoch 10\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 11\n","Learning rate_agent : 0.0005987369392383785\n","Learning rate_usr : 0.0005987369392383785\n","Agent evaluation!\n","togrep : results : epoch 11 ; accuracy valid : 11.2, precision@10 valid : 73.16\n","User model evaluation!\n","togrep : results : epoch 11 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 11 ; mean accuracy pred valid : 3.5, map pred valid : 11.52; mean accuracy reward valid : 58.52\n","saving model at epoch 11\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 12\n","Learning rate_agent : 0.0005688000922764595\n","Learning rate_usr : 0.0005688000922764595\n","Agent evaluation!\n","togrep : results : epoch 12 ; accuracy valid : 11.24, precision@10 valid : 73.35\n","User model evaluation!\n","togrep : results : epoch 12 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 12 ; mean accuracy pred valid : 3.53, map pred valid : 11.53; mean accuracy reward valid : 58.52\n","saving model at epoch 12\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 13\n","Learning rate_agent : 0.0005403600876626365\n","Learning rate_usr : 0.0005403600876626365\n","Agent evaluation!\n","togrep : results : epoch 13 ; accuracy valid : 11.24, precision@10 valid : 73.16\n","User model evaluation!\n","togrep : results : epoch 13 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 13 ; mean accuracy pred valid : 3.5, map pred valid : 11.5; mean accuracy reward valid : 58.52\n","saving model at epoch 13\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 14\n","Learning rate_agent : 0.0005133420832795047\n","Learning rate_usr : 0.0005133420832795047\n","Agent evaluation!\n","togrep : results : epoch 14 ; accuracy valid : 11.36, precision@10 valid : 73.25\n","User model evaluation!\n","togrep : results : epoch 14 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 14 ; mean accuracy pred valid : 3.5, map pred valid : 11.52; mean accuracy reward valid : 58.52\n","saving model at epoch 14\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 15\n","Learning rate_agent : 0.00048767497911552944\n","Learning rate_usr : 0.00048767497911552944\n","Agent evaluation!\n","togrep : results : epoch 15 ; accuracy valid : 11.45, precision@10 valid : 73.13\n","User model evaluation!\n","togrep : results : epoch 15 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 15 ; mean accuracy pred valid : 3.5, map pred valid : 11.5; mean accuracy reward valid : 58.52\n","saving model at epoch 15\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 16\n","Learning rate_agent : 0.00046329123015975297\n","Learning rate_usr : 0.00046329123015975297\n","Agent evaluation!\n","togrep : results : epoch 16 ; accuracy valid : 11.52, precision@10 valid : 73.47\n","User model evaluation!\n","togrep : results : epoch 16 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 16 ; mean accuracy pred valid : 3.53, map pred valid : 11.51; mean accuracy reward valid : 58.52\n","saving model at epoch 16\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 17\n","Learning rate_agent : 0.0004401266686517653\n","Learning rate_usr : 0.0004401266686517653\n","Agent evaluation!\n","togrep : results : epoch 17 ; accuracy valid : 11.11, precision@10 valid : 73.47\n","User model evaluation!\n","togrep : results : epoch 17 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 17 ; mean accuracy pred valid : 3.53, map pred valid : 11.54; mean accuracy reward valid : 58.52\n","saving model at epoch 17\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 18\n","Learning rate_agent : 0.00041812033521917703\n","Learning rate_usr : 0.00041812033521917703\n","Agent evaluation!\n","togrep : results : epoch 18 ; accuracy valid : 11.14, precision@10 valid : 73.56\n","User model evaluation!\n","togrep : results : epoch 18 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 18 ; mean accuracy pred valid : 3.53, map pred valid : 11.53; mean accuracy reward valid : 58.52\n","saving model at epoch 18\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 19\n","Learning rate_agent : 0.00039721431845821814\n","Learning rate_usr : 0.00039721431845821814\n","Agent evaluation!\n","togrep : results : epoch 19 ; accuracy valid : 11.24, precision@10 valid : 73.53\n","User model evaluation!\n","togrep : results : epoch 19 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 19 ; mean accuracy pred valid : 3.53, map pred valid : 11.52; mean accuracy reward valid : 58.52\n","saving model at epoch 19\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 20\n","Learning rate_agent : 0.0003773536025353072\n","Learning rate_usr : 0.0003773536025353072\n","Agent evaluation!\n","togrep : results : epoch 20 ; accuracy valid : 11.24, precision@10 valid : 73.38\n","User model evaluation!\n","togrep : results : epoch 20 ; accuracy valid : 15.14, precision@10 valid : 72.75, reward_accuracy valid 58.52\n","Interaction evaluation!\n","togrep : results : epoch 20 ; mean accuracy pred valid : 3.53, map pred valid : 11.51; mean accuracy reward valid : 58.52\n","saving model at epoch 20\n","Testing\n","Agent evaluation!\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; accuracy test : 10.92, precision@10 test : 72.13\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:225: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  trainSample.genSample_generator(np.array(clicklist)[trainindex].tolist(), np.array(rewardlist)[trainindex].tolist(), np.array(actionlist)[trainindex].tolist(), real_num_label, False)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:226: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  validSample.genSample_generator(np.array(clicklist)[validindex].tolist(), np.array(rewardlist)[validindex].tolist(), np.array(actionlist)[validindex].tolist(), real_num_label, False)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  testSample.genSample_generator(np.array(clicklist)[testindex].tolist(), np.array(rewardlist)[testindex].tolist(), np.array(actionlist)[testindex].tolist(), real_num_label, False)\n","Generate sample : 8000\n","\n","--------------------------------------------\n","Pretrain the Discriminator\n","--------------------------------------------\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:230: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  trainSample.genSample_discriminator(np.array(clicklist)[trainindex].tolist(), np.array(rewardlist)[trainindex].tolist(), np.array(actionlist)[trainindex].tolist(), np.array(targetlist)[trainindex].tolist(), real_num_label)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:231: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  validSample.genSample_discriminator(np.array(clicklist)[validindex].tolist(), np.array(rewardlist)[validindex].tolist(), np.array(actionlist)[validindex].tolist(), np.array(targetlist)[validindex].tolist(), real_num_label)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:232: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  testSample.genSample_discriminator(np.array(clicklist)[testindex].tolist(), np.array(rewardlist)[testindex].tolist(), np.array(actionlist)[testindex].tolist(), np.array(targetlist)[testindex].tolist(), real_num_label)\n","Train sample : 12800\n","Valid sample : 1600\n","Test sample : 1600\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 96.19, map valid : 98.09\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 97.5, map valid : 98.75\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 97.69, map valid : 98.84\n","saving model at epoch 3\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 97.88, map valid : 98.94\n","saving model at epoch 4\n","\n","DISCRIMINATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","togrep : results : epoch 5 ; mean accuracy valid : 98.06, map valid : 99.03\n","saving model at epoch 5\n","Testing\n","Discriminator evaluation!\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; mean accuracy test : 97.56, map test : 98.78\n","\n","--------------------------------------------\n","Adversarial Training\n","--------------------------------------------\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; accuracy test : 10.92, precision@10 test : 72.13\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 1\n","Learning rate_agent : 0.001\n","Learning rate_usr : 0.001\n","Agent evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 12.05, precision@10 valid : 73.6\n","User model evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 14.01, precision@10 valid : 73.06, reward_accuracy valid 57.18\n","Interaction evaluation!\n","togrep : results : epoch 1 ; mean accuracy pred valid : 3.09, map pred valid : 11.07; mean accuracy reward valid : 57.18\n","saving model at epoch 1\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 95.75, map valid : 97.88\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 97.88, map valid : 98.94\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.12, map valid : 99.06\n","saving model at epoch 3\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 98.5, map valid : 99.25\n","saving model at epoch 4\n","\n","DISCRIMINATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","togrep : results : epoch 5 ; mean accuracy valid : 98.56, map valid : 99.28\n","saving model at epoch 5\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 2\n","Learning rate_agent : 0.00095\n","Learning rate_usr : 0.00095\n","Agent evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 11.92, precision@10 valid : 73.53\n","User model evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 15.32, precision@10 valid : 74.97, reward_accuracy valid 56.55\n","Interaction evaluation!\n","togrep : results : epoch 2 ; mean accuracy pred valid : 3.03, map pred valid : 11.17; mean accuracy reward valid : 56.55\n","saving model at epoch 2\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 95.38, map valid : 97.69\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 96.25, map valid : 98.12\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 96.25, map valid : 98.12\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 96.31, map valid : 98.16\n","saving model at epoch 4\n","\n","DISCRIMINATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","togrep : results : epoch 5 ; mean accuracy valid : 96.5, map valid : 98.25\n","saving model at epoch 5\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 3\n","Learning rate_agent : 0.0009025\n","Learning rate_usr : 0.0009025\n","Agent evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 11.95, precision@10 valid : 73.44\n","User model evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 15.2, precision@10 valid : 75.22, reward_accuracy valid 56.49\n","Interaction evaluation!\n","togrep : results : epoch 3 ; mean accuracy pred valid : 3.15, map pred valid : 11.35; mean accuracy reward valid : 56.49\n","saving model at epoch 3\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 96.62, map valid : 98.31\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 96.56, map valid : 98.28\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 96.56, map valid : 98.28\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 4\n","Learning rate_agent : 0.000857375\n","Learning rate_usr : 0.000857375\n","Agent evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 11.61, precision@10 valid : 73.44\n","User model evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 15.39, precision@10 valid : 75.34, reward_accuracy valid 56.49\n","Interaction evaluation!\n","togrep : results : epoch 4 ; mean accuracy pred valid : 3.25, map pred valid : 11.41; mean accuracy reward valid : 56.49\n","saving model at epoch 4\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 96.81, map valid : 98.41\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 96.88, map valid : 98.44\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 96.75, map valid : 98.38\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 96.94, map valid : 98.47\n","saving model at epoch 4\n","\n","DISCRIMINATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","togrep : results : epoch 5 ; mean accuracy valid : 96.81, map valid : 98.41\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 5\n","Learning rate_agent : 0.0008145062499999999\n","Learning rate_usr : 0.0008145062499999999\n","Agent evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 11.27, precision@10 valid : 73.56\n","User model evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 15.7, precision@10 valid : 75.09, reward_accuracy valid 56.49\n","Interaction evaluation!\n","togrep : results : epoch 5 ; mean accuracy pred valid : 3.37, map pred valid : 11.54; mean accuracy reward valid : 56.49\n","saving model at epoch 5\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 96.25, map valid : 98.12\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 96.0, map valid : 98.0\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 95.5, map valid : 97.75\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 6\n","Learning rate_agent : 0.0007737809374999998\n","Learning rate_usr : 0.0007737809374999998\n","Agent evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 11.14, precision@10 valid : 73.63\n","User model evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 15.61, precision@10 valid : 75.56, reward_accuracy valid 56.49\n","Interaction evaluation!\n","togrep : results : epoch 6 ; mean accuracy pred valid : 3.53, map pred valid : 11.67; mean accuracy reward valid : 56.49\n","saving model at epoch 6\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 97.38, map valid : 98.69\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 97.0, map valid : 98.5\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 97.06, map valid : 98.53\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 7\n","Learning rate_agent : 0.0007350918906249997\n","Learning rate_usr : 0.0007350918906249997\n","Agent evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 11.08, precision@10 valid : 73.44\n","User model evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 15.7, precision@10 valid : 75.75, reward_accuracy valid 56.49\n","Interaction evaluation!\n","togrep : results : epoch 7 ; mean accuracy pred valid : 3.59, map pred valid : 11.82; mean accuracy reward valid : 56.49\n","saving model at epoch 7\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 97.06, map valid : 98.53\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 97.0, map valid : 98.5\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 96.88, map valid : 98.44\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 8\n","Learning rate_agent : 0.0006983372960937497\n","Learning rate_usr : 0.0006983372960937497\n","Agent evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 11.11, precision@10 valid : 73.28\n","User model evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 15.67, precision@10 valid : 75.72, reward_accuracy valid 56.59\n","Interaction evaluation!\n","togrep : results : epoch 8 ; mean accuracy pred valid : 3.56, map pred valid : 11.81; mean accuracy reward valid : 56.59\n","saving model at epoch 8\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 96.69, map valid : 98.34\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 96.62, map valid : 98.31\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 96.5, map valid : 98.25\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 9\n","Learning rate_agent : 0.0006634204312890621\n","Learning rate_usr : 0.0006634204312890621\n","Agent evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 11.27, precision@10 valid : 73.22\n","User model evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 15.79, precision@10 valid : 75.66, reward_accuracy valid 56.49\n","Interaction evaluation!\n","togrep : results : epoch 9 ; mean accuracy pred valid : 3.56, map pred valid : 11.78; mean accuracy reward valid : 56.49\n","saving model at epoch 9\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 97.06, map valid : 98.53\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 97.38, map valid : 98.69\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 97.38, map valid : 98.69\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 97.38, map valid : 98.69\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 10\n","Learning rate_agent : 0.000630249409724609\n","Learning rate_usr : 0.000630249409724609\n","Agent evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 10.89, precision@10 valid : 73.03\n","User model evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 15.92, precision@10 valid : 75.66, reward_accuracy valid 56.49\n","Interaction evaluation!\n","togrep : results : epoch 10 ; mean accuracy pred valid : 3.53, map pred valid : 11.84; mean accuracy reward valid : 56.49\n","saving model at epoch 10\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 96.94, map valid : 98.47\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 96.75, map valid : 98.38\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 96.81, map valid : 98.41\n","Testing\n","Agent evaluation!\n","\n","VALIDATION : Epoch 101\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:153: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return np.array(self.itemSeq[batchstart:batchend]), np.array(self.target[batchstart:batchend]), np.array(self.reward[batchstart:batchend]), np.array(self.action[batchstart:batchend])\n","togrep : results : epoch 101 ; accuracy test : 10.71, precision@10 test : 72.47\n","User model evaluation!\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; accuracy test : 15.45, precision@10 test : 74.38, reward_accuracy test 54.99\n","The original reward is:1.6371\n","The optimal reward is:1.9584\n","\n","The original reward is: 1.7191\n","==============================================\n","Training on the epoch:1\n","==============================================\n","100\n","Train seq : 8000\n","Valid seq : 1000\n","Test seq : 1000\n","20\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:215: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  trainSample.genSample_pred(np.array(clicklist)[trainindex].tolist(), np.array(rewardlist)[trainindex].tolist(), np.array(actionlist)[trainindex].tolist(), warm_up, real_num_label, rec_len, add_end = True)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:216: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  validSample.genSample_predtest(np.array(clicklist)[validindex].tolist(),np.array(rewardlist)[validindex].tolist(), np.array(actionlist)[validindex].tolist(), warm_up, real_num_label, add_end = False)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:217: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  testSample.genSample_predtest(np.array(clicklist)[testindex].tolist(),np.array(rewardlist)[testindex].tolist(), np.array(actionlist)[testindex].tolist(), warm_up, real_num_label, add_end = False)\n","\n","--------------------------------------------\n","Pretrain Generator with given recommendation\n","--------------------------------------------\n","\n","GENERATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:153: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return np.array(self.itemSeq[batchstart:batchend]), np.array(self.target[batchstart:batchend]), np.array(self.reward[batchstart:batchend]), np.array(self.action[batchstart:batchend])\n","results : epoch 1 ; mean accuracy pred : 22.92; mean P@10 pred: 62.54; mean accuracy reward: 64.14\n","User model evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 6.86, precision@10 valid : 55.52, reward_accuracy valid 57.17\n","saving model at epoch 1\n","\n","GENERATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","results : epoch 2 ; mean accuracy pred : 24.99; mean P@10 pred: 62.04; mean accuracy reward: 66.88\n","User model evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 8.8, precision@10 valid : 55.14, reward_accuracy valid 57.95\n","saving model at epoch 2\n","\n","GENERATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","results : epoch 3 ; mean accuracy pred : 26.65; mean P@10 pred: 61.01; mean accuracy reward: 67.28\n","User model evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 10.31, precision@10 valid : 54.56, reward_accuracy valid 58.27\n","saving model at epoch 3\n","\n","GENERATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","results : epoch 4 ; mean accuracy pred : 27.02; mean P@10 pred: 60.97; mean accuracy reward: 67.39\n","User model evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 10.37, precision@10 valid : 55.08, reward_accuracy valid 58.27\n","saving model at epoch 4\n","\n","GENERATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","results : epoch 5 ; mean accuracy pred : 27.02; mean P@10 pred: 61.0; mean accuracy reward: 67.44\n","User model evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 10.48, precision@10 valid : 55.05, reward_accuracy valid 58.3\n","saving model at epoch 5\n","\n","GENERATOR TRAINING : Epoch 6\n","Learning rate : 0.0007737809374999998\n","results : epoch 6 ; mean accuracy pred : 27.1; mean P@10 pred: 61.17; mean accuracy reward: 67.49\n","User model evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 10.6, precision@10 valid : 54.94, reward_accuracy valid 58.41\n","saving model at epoch 6\n","\n","GENERATOR TRAINING : Epoch 7\n","Learning rate : 0.0007350918906249997\n","results : epoch 7 ; mean accuracy pred : 27.19; mean P@10 pred: 61.28; mean accuracy reward: 67.48\n","User model evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 10.72, precision@10 valid : 54.88, reward_accuracy valid 58.38\n","saving model at epoch 7\n","\n","GENERATOR TRAINING : Epoch 8\n","Learning rate : 0.0006983372960937497\n","results : epoch 8 ; mean accuracy pred : 27.28; mean P@10 pred: 61.62; mean accuracy reward: 67.49\n","User model evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 10.72, precision@10 valid : 55.05, reward_accuracy valid 58.27\n","saving model at epoch 8\n","\n","GENERATOR TRAINING : Epoch 9\n","Learning rate : 0.0006634204312890621\n","results : epoch 9 ; mean accuracy pred : 27.29; mean P@10 pred: 61.92; mean accuracy reward: 67.49\n","User model evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 10.77, precision@10 valid : 55.26, reward_accuracy valid 58.27\n","saving model at epoch 9\n","\n","GENERATOR TRAINING : Epoch 10\n","Learning rate : 0.000630249409724609\n","results : epoch 10 ; mean accuracy pred : 27.3; mean P@10 pred: 62.18; mean accuracy reward: 67.51\n","User model evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 10.89, precision@10 valid : 55.43, reward_accuracy valid 58.12\n","saving model at epoch 10\n","\n","GENERATOR TRAINING : Epoch 11\n","Learning rate : 0.0005987369392383785\n","results : epoch 11 ; mean accuracy pred : 27.35; mean P@10 pred: 62.5; mean accuracy reward: 67.47\n","User model evaluation!\n","togrep : results : epoch 11 ; accuracy valid : 11.0, precision@10 valid : 56.04, reward_accuracy valid 58.18\n","saving model at epoch 11\n","\n","GENERATOR TRAINING : Epoch 12\n","Learning rate : 0.0005688000922764595\n","results : epoch 12 ; mean accuracy pred : 27.44; mean P@10 pred: 63.0; mean accuracy reward: 67.54\n","User model evaluation!\n","togrep : results : epoch 12 ; accuracy valid : 11.18, precision@10 valid : 57.02, reward_accuracy valid 58.09\n","saving model at epoch 12\n","\n","GENERATOR TRAINING : Epoch 13\n","Learning rate : 0.0005403600876626365\n","results : epoch 13 ; mean accuracy pred : 27.49; mean P@10 pred: 63.27; mean accuracy reward: 67.52\n","User model evaluation!\n","togrep : results : epoch 13 ; accuracy valid : 11.21, precision@10 valid : 57.11, reward_accuracy valid 58.09\n","saving model at epoch 13\n","\n","GENERATOR TRAINING : Epoch 14\n","Learning rate : 0.0005133420832795047\n","results : epoch 14 ; mean accuracy pred : 27.51; mean P@10 pred: 63.41; mean accuracy reward: 67.53\n","User model evaluation!\n","togrep : results : epoch 14 ; accuracy valid : 11.06, precision@10 valid : 57.23, reward_accuracy valid 58.15\n","saving model at epoch 14\n","\n","GENERATOR TRAINING : Epoch 15\n","Learning rate : 0.00048767497911552944\n","results : epoch 15 ; mean accuracy pred : 27.52; mean P@10 pred: 63.5; mean accuracy reward: 67.53\n","User model evaluation!\n","togrep : results : epoch 15 ; accuracy valid : 10.86, precision@10 valid : 57.11, reward_accuracy valid 58.12\n","\n","GENERATOR TRAINING : Epoch 16\n","Learning rate : 0.00046329123015975297\n","results : epoch 16 ; mean accuracy pred : 27.51; mean P@10 pred: 63.62; mean accuracy reward: 67.53\n","User model evaluation!\n","togrep : results : epoch 16 ; accuracy valid : 10.8, precision@10 valid : 57.34, reward_accuracy valid 58.12\n","saving model at epoch 16\n","\n","GENERATOR TRAINING : Epoch 17\n","Learning rate : 0.0004401266686517653\n","results : epoch 17 ; mean accuracy pred : 27.51; mean P@10 pred: 63.7; mean accuracy reward: 67.57\n","User model evaluation!\n","togrep : results : epoch 17 ; accuracy valid : 10.77, precision@10 valid : 57.34, reward_accuracy valid 58.09\n","\n","GENERATOR TRAINING : Epoch 18\n","Learning rate : 0.00041812033521917703\n","results : epoch 18 ; mean accuracy pred : 27.52; mean P@10 pred: 63.73; mean accuracy reward: 67.57\n","User model evaluation!\n","togrep : results : epoch 18 ; accuracy valid : 10.77, precision@10 valid : 57.49, reward_accuracy valid 58.09\n","saving model at epoch 18\n","\n","GENERATOR TRAINING : Epoch 19\n","Learning rate : 0.00039721431845821814\n","results : epoch 19 ; mean accuracy pred : 27.52; mean P@10 pred: 63.86; mean accuracy reward: 67.58\n","User model evaluation!\n","togrep : results : epoch 19 ; accuracy valid : 10.8, precision@10 valid : 57.52, reward_accuracy valid 58.12\n","saving model at epoch 19\n","\n","GENERATOR TRAINING : Epoch 20\n","Learning rate : 0.0003773536025353072\n","results : epoch 20 ; mean accuracy pred : 27.53; mean P@10 pred: 63.94; mean accuracy reward: 67.55\n","User model evaluation!\n","togrep : results : epoch 20 ; accuracy valid : 10.86, precision@10 valid : 57.54, reward_accuracy valid 58.04\n","saving model at epoch 20\n","\n","GENERATOR TRAINING : Epoch 21\n","Learning rate : 0.0003584859224085418\n","results : epoch 21 ; mean accuracy pred : 27.53; mean P@10 pred: 63.96; mean accuracy reward: 67.56\n","User model evaluation!\n","togrep : results : epoch 21 ; accuracy valid : 10.86, precision@10 valid : 57.52, reward_accuracy valid 58.04\n","\n","GENERATOR TRAINING : Epoch 22\n","Learning rate : 0.0003405616262881147\n","results : epoch 22 ; mean accuracy pred : 27.53; mean P@10 pred: 64.06; mean accuracy reward: 67.57\n","User model evaluation!\n","togrep : results : epoch 22 ; accuracy valid : 10.92, precision@10 valid : 57.63, reward_accuracy valid 58.04\n","saving model at epoch 22\n","\n","GENERATOR TRAINING : Epoch 23\n","Learning rate : 0.00032353354497370894\n","results : epoch 23 ; mean accuracy pred : 27.53; mean P@10 pred: 64.16; mean accuracy reward: 67.57\n","User model evaluation!\n","togrep : results : epoch 23 ; accuracy valid : 10.83, precision@10 valid : 57.69, reward_accuracy valid 58.04\n","saving model at epoch 23\n","\n","GENERATOR TRAINING : Epoch 24\n","Learning rate : 0.00030735686772502346\n","results : epoch 24 ; mean accuracy pred : 27.52; mean P@10 pred: 64.33; mean accuracy reward: 67.58\n","User model evaluation!\n","togrep : results : epoch 24 ; accuracy valid : 10.86, precision@10 valid : 57.66, reward_accuracy valid 58.04\n","saving model at epoch 24\n","\n","GENERATOR TRAINING : Epoch 25\n","Learning rate : 0.00029198902433877225\n","results : epoch 25 ; mean accuracy pred : 27.55; mean P@10 pred: 64.36; mean accuracy reward: 67.55\n","User model evaluation!\n","togrep : results : epoch 25 ; accuracy valid : 10.95, precision@10 valid : 57.89, reward_accuracy valid 58.12\n","saving model at epoch 25\n","\n","GENERATOR TRAINING : Epoch 26\n","Learning rate : 0.00027738957312183364\n","results : epoch 26 ; mean accuracy pred : 27.57; mean P@10 pred: 64.42; mean accuracy reward: 67.54\n","User model evaluation!\n","togrep : results : epoch 26 ; accuracy valid : 11.0, precision@10 valid : 57.98, reward_accuracy valid 58.12\n","saving model at epoch 26\n","\n","GENERATOR TRAINING : Epoch 27\n","Learning rate : 0.0002635200944657419\n","results : epoch 27 ; mean accuracy pred : 27.57; mean P@10 pred: 64.43; mean accuracy reward: 67.58\n","User model evaluation!\n","togrep : results : epoch 27 ; accuracy valid : 11.03, precision@10 valid : 57.95, reward_accuracy valid 58.12\n","saving model at epoch 27\n","\n","GENERATOR TRAINING : Epoch 28\n","Learning rate : 0.0002503440897424548\n","results : epoch 28 ; mean accuracy pred : 27.57; mean P@10 pred: 64.47; mean accuracy reward: 67.59\n","User model evaluation!\n","togrep : results : epoch 28 ; accuracy valid : 11.03, precision@10 valid : 58.07, reward_accuracy valid 58.12\n","saving model at epoch 28\n","\n","GENERATOR TRAINING : Epoch 29\n","Learning rate : 0.00023782688525533205\n","results : epoch 29 ; mean accuracy pred : 27.57; mean P@10 pred: 64.49; mean accuracy reward: 67.6\n","User model evaluation!\n","togrep : results : epoch 29 ; accuracy valid : 11.03, precision@10 valid : 58.07, reward_accuracy valid 58.12\n","\n","GENERATOR TRAINING : Epoch 30\n","Learning rate : 0.00022593554099256544\n","results : epoch 30 ; mean accuracy pred : 27.57; mean P@10 pred: 64.54; mean accuracy reward: 67.6\n","User model evaluation!\n","togrep : results : epoch 30 ; accuracy valid : 11.03, precision@10 valid : 58.09, reward_accuracy valid 58.09\n","saving model at epoch 30\n","\n","GENERATOR TRAINING : Epoch 31\n","Learning rate : 0.00021463876394293716\n","results : epoch 31 ; mean accuracy pred : 27.57; mean P@10 pred: 64.56; mean accuracy reward: 67.61\n","User model evaluation!\n","togrep : results : epoch 31 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","saving model at epoch 31\n","\n","GENERATOR TRAINING : Epoch 32\n","Learning rate : 0.0002039068257457903\n","results : epoch 32 ; mean accuracy pred : 27.56; mean P@10 pred: 64.61; mean accuracy reward: 67.61\n","User model evaluation!\n","togrep : results : epoch 32 ; accuracy valid : 10.98, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","\n","GENERATOR TRAINING : Epoch 33\n","Learning rate : 0.00019371148445850077\n","results : epoch 33 ; mean accuracy pred : 27.57; mean P@10 pred: 64.62; mean accuracy reward: 67.63\n","User model evaluation!\n","togrep : results : epoch 33 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Testing\n","User model evaluation!\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; accuracy test : 11.56, precision@10 test : 57.42, reward_accuracy test 56.96\n","\n","--------------------------------------------\n","Pretrain by the adversarial training\n","---------------------------------------------\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:220: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  trainSample.genSample_generator(np.array(clicklist)[trainindex].tolist(), np.array(rewardlist)[trainindex].tolist(), np.array(actionlist)[trainindex].tolist(), real_num_label, rec_len, add_end = True)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:221: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  validSample.genSample_predtest(np.array(clicklist)[validindex].tolist(),np.array(rewardlist)[validindex].tolist(), np.array(actionlist)[validindex].tolist(), warm_up, real_num_label, add_end = False)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:222: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  testSample.genSample_predtest(np.array(clicklist)[testindex].tolist(),np.array(rewardlist)[testindex].tolist(), np.array(actionlist)[testindex].tolist(), warm_up, real_num_label, add_end = False)\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 1\n","Learning rate_agent : 0.001\n","Learning rate_usr : 0.001\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:158: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  self.itemSeq = np.array(origin.itemSeq)[index[:subnum]].tolist()\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  self.reward = np.array(origin.reward)[index[:subnum]].tolist()\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  self.action = np.array(origin.action)[index[:subnum]].tolist()\n","Agent evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 6.83, precision@10 valid : 58.3\n","User model evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 1 ; mean accuracy pred valid : 3.48, map pred valid : 9.23; mean accuracy reward valid : 58.09\n","saving model at epoch 1\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 2\n","Learning rate_agent : 0.00095\n","Learning rate_usr : 0.00095\n","Agent evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 8.72, precision@10 valid : 61.45\n","User model evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 2 ; mean accuracy pred valid : 4.11, map pred valid : 10.95; mean accuracy reward valid : 58.09\n","saving model at epoch 2\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 3\n","Learning rate_agent : 0.0009025\n","Learning rate_usr : 0.0009025\n","Agent evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 9.73, precision@10 valid : 64.35\n","User model evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 3 ; mean accuracy pred valid : 4.08, map pred valid : 11.19; mean accuracy reward valid : 58.09\n","saving model at epoch 3\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 4\n","Learning rate_agent : 0.000857375\n","Learning rate_usr : 0.000857375\n","Agent evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 10.22, precision@10 valid : 65.68\n","User model evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 4 ; mean accuracy pred valid : 4.08, map pred valid : 11.35; mean accuracy reward valid : 58.09\n","saving model at epoch 4\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 5\n","Learning rate_agent : 0.0008145062499999999\n","Learning rate_usr : 0.0008145062499999999\n","Agent evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 10.25, precision@10 valid : 66.2\n","User model evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 5 ; mean accuracy pred valid : 4.08, map pred valid : 11.33; mean accuracy reward valid : 58.09\n","saving model at epoch 5\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 6\n","Learning rate_agent : 0.0007737809374999998\n","Learning rate_usr : 0.0007737809374999998\n","Agent evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 10.31, precision@10 valid : 66.78\n","User model evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 6 ; mean accuracy pred valid : 4.08, map pred valid : 11.35; mean accuracy reward valid : 58.09\n","saving model at epoch 6\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 7\n","Learning rate_agent : 0.0007350918906249997\n","Learning rate_usr : 0.0007350918906249997\n","Agent evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 10.25, precision@10 valid : 67.19\n","User model evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 7 ; mean accuracy pred valid : 4.08, map pred valid : 11.37; mean accuracy reward valid : 58.09\n","saving model at epoch 7\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 8\n","Learning rate_agent : 0.0006983372960937497\n","Learning rate_usr : 0.0006983372960937497\n","Agent evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 10.25, precision@10 valid : 67.62\n","User model evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 8 ; mean accuracy pred valid : 4.08, map pred valid : 11.43; mean accuracy reward valid : 58.09\n","saving model at epoch 8\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 9\n","Learning rate_agent : 0.0006634204312890621\n","Learning rate_usr : 0.0006634204312890621\n","Agent evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 10.43, precision@10 valid : 67.83\n","User model evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 9 ; mean accuracy pred valid : 4.08, map pred valid : 11.39; mean accuracy reward valid : 58.09\n","saving model at epoch 9\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 10\n","Learning rate_agent : 0.000630249409724609\n","Learning rate_usr : 0.000630249409724609\n","Agent evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 10.45, precision@10 valid : 68.0\n","User model evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 10 ; mean accuracy pred valid : 4.08, map pred valid : 11.52; mean accuracy reward valid : 58.09\n","saving model at epoch 10\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 11\n","Learning rate_agent : 0.0005987369392383785\n","Learning rate_usr : 0.0005987369392383785\n","Agent evaluation!\n","togrep : results : epoch 11 ; accuracy valid : 10.66, precision@10 valid : 68.43\n","User model evaluation!\n","togrep : results : epoch 11 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 11 ; mean accuracy pred valid : 4.11, map pred valid : 11.55; mean accuracy reward valid : 58.09\n","saving model at epoch 11\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 12\n","Learning rate_agent : 0.0005688000922764595\n","Learning rate_usr : 0.0005688000922764595\n","Agent evaluation!\n","togrep : results : epoch 12 ; accuracy valid : 10.69, precision@10 valid : 68.81\n","User model evaluation!\n","togrep : results : epoch 12 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 12 ; mean accuracy pred valid : 4.14, map pred valid : 11.57; mean accuracy reward valid : 58.09\n","saving model at epoch 12\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 13\n","Learning rate_agent : 0.0005403600876626365\n","Learning rate_usr : 0.0005403600876626365\n","Agent evaluation!\n","togrep : results : epoch 13 ; accuracy valid : 10.77, precision@10 valid : 69.24\n","User model evaluation!\n","togrep : results : epoch 13 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 13 ; mean accuracy pred valid : 4.14, map pred valid : 11.58; mean accuracy reward valid : 58.09\n","saving model at epoch 13\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 14\n","Learning rate_agent : 0.0005133420832795047\n","Learning rate_usr : 0.0005133420832795047\n","Agent evaluation!\n","togrep : results : epoch 14 ; accuracy valid : 10.69, precision@10 valid : 69.48\n","User model evaluation!\n","togrep : results : epoch 14 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 14 ; mean accuracy pred valid : 4.14, map pred valid : 11.58; mean accuracy reward valid : 58.09\n","saving model at epoch 14\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 15\n","Learning rate_agent : 0.00048767497911552944\n","Learning rate_usr : 0.00048767497911552944\n","Agent evaluation!\n","togrep : results : epoch 15 ; accuracy valid : 10.72, precision@10 valid : 69.53\n","User model evaluation!\n","togrep : results : epoch 15 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 15 ; mean accuracy pred valid : 4.14, map pred valid : 11.59; mean accuracy reward valid : 58.09\n","saving model at epoch 15\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 16\n","Learning rate_agent : 0.00046329123015975297\n","Learning rate_usr : 0.00046329123015975297\n","Agent evaluation!\n","togrep : results : epoch 16 ; accuracy valid : 10.69, precision@10 valid : 69.85\n","User model evaluation!\n","togrep : results : epoch 16 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 16 ; mean accuracy pred valid : 4.14, map pred valid : 11.6; mean accuracy reward valid : 58.09\n","saving model at epoch 16\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 17\n","Learning rate_agent : 0.0004401266686517653\n","Learning rate_usr : 0.0004401266686517653\n","Agent evaluation!\n","togrep : results : epoch 17 ; accuracy valid : 10.74, precision@10 valid : 69.94\n","User model evaluation!\n","togrep : results : epoch 17 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 17 ; mean accuracy pred valid : 4.14, map pred valid : 11.6; mean accuracy reward valid : 58.09\n","saving model at epoch 17\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 18\n","Learning rate_agent : 0.00041812033521917703\n","Learning rate_usr : 0.00041812033521917703\n","Agent evaluation!\n","togrep : results : epoch 18 ; accuracy valid : 10.72, precision@10 valid : 70.0\n","User model evaluation!\n","togrep : results : epoch 18 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 18 ; mean accuracy pred valid : 4.14, map pred valid : 11.62; mean accuracy reward valid : 58.09\n","saving model at epoch 18\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 19\n","Learning rate_agent : 0.00039721431845821814\n","Learning rate_usr : 0.00039721431845821814\n","Agent evaluation!\n","togrep : results : epoch 19 ; accuracy valid : 10.69, precision@10 valid : 70.0\n","User model evaluation!\n","togrep : results : epoch 19 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 19 ; mean accuracy pred valid : 4.11, map pred valid : 11.59; mean accuracy reward valid : 58.09\n","saving model at epoch 19\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 20\n","Learning rate_agent : 0.0003773536025353072\n","Learning rate_usr : 0.0003773536025353072\n","Agent evaluation!\n","togrep : results : epoch 20 ; accuracy valid : 10.77, precision@10 valid : 69.94\n","User model evaluation!\n","togrep : results : epoch 20 ; accuracy valid : 11.0, precision@10 valid : 58.18, reward_accuracy valid 58.09\n","Interaction evaluation!\n","togrep : results : epoch 20 ; mean accuracy pred valid : 4.11, map pred valid : 11.64; mean accuracy reward valid : 58.09\n","saving model at epoch 20\n","Testing\n","Agent evaluation!\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; accuracy test : 9.79, precision@10 test : 69.39\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:225: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  trainSample.genSample_generator(np.array(clicklist)[trainindex].tolist(), np.array(rewardlist)[trainindex].tolist(), np.array(actionlist)[trainindex].tolist(), real_num_label, False)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:226: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  validSample.genSample_generator(np.array(clicklist)[validindex].tolist(), np.array(rewardlist)[validindex].tolist(), np.array(actionlist)[validindex].tolist(), real_num_label, False)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  testSample.genSample_generator(np.array(clicklist)[testindex].tolist(), np.array(rewardlist)[testindex].tolist(), np.array(actionlist)[testindex].tolist(), real_num_label, False)\n","Generate sample : 8000\n","\n","--------------------------------------------\n","Pretrain the Discriminator\n","--------------------------------------------\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:230: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  trainSample.genSample_discriminator(np.array(clicklist)[trainindex].tolist(), np.array(rewardlist)[trainindex].tolist(), np.array(actionlist)[trainindex].tolist(), np.array(targetlist)[trainindex].tolist(), real_num_label)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:231: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  validSample.genSample_discriminator(np.array(clicklist)[validindex].tolist(), np.array(rewardlist)[validindex].tolist(), np.array(actionlist)[validindex].tolist(), np.array(targetlist)[validindex].tolist(), real_num_label)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:232: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  testSample.genSample_discriminator(np.array(clicklist)[testindex].tolist(), np.array(rewardlist)[testindex].tolist(), np.array(actionlist)[testindex].tolist(), np.array(targetlist)[testindex].tolist(), real_num_label)\n","Train sample : 12800\n","Valid sample : 1600\n","Test sample : 1600\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 95.62, map valid : 97.81\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 95.94, map valid : 97.97\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 96.19, map valid : 98.09\n","saving model at epoch 3\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 96.38, map valid : 98.19\n","saving model at epoch 4\n","\n","DISCRIMINATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","togrep : results : epoch 5 ; mean accuracy valid : 96.31, map valid : 98.16\n","Testing\n","Discriminator evaluation!\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; mean accuracy test : 96.25, map test : 98.12\n","\n","--------------------------------------------\n","Adversarial Training\n","--------------------------------------------\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; accuracy test : 9.79, precision@10 test : 69.39\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 1\n","Learning rate_agent : 0.001\n","Learning rate_usr : 0.001\n","Agent evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 11.0, precision@10 valid : 69.97\n","User model evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 11.53, precision@10 valid : 60.64, reward_accuracy valid 56.68\n","Interaction evaluation!\n","togrep : results : epoch 1 ; mean accuracy pred valid : 4.37, map pred valid : 11.72; mean accuracy reward valid : 56.68\n","saving model at epoch 1\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 86.19, map valid : 93.09\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 93.38, map valid : 96.69\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 95.25, map valid : 97.62\n","saving model at epoch 3\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 95.81, map valid : 97.91\n","saving model at epoch 4\n","\n","DISCRIMINATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","togrep : results : epoch 5 ; mean accuracy valid : 96.0, map valid : 98.0\n","saving model at epoch 5\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 2\n","Learning rate_agent : 0.00095\n","Learning rate_usr : 0.00095\n","Agent evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 10.89, precision@10 valid : 70.29\n","User model evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 11.58, precision@10 valid : 61.8, reward_accuracy valid 56.56\n","Interaction evaluation!\n","togrep : results : epoch 2 ; mean accuracy pred valid : 4.43, map pred valid : 11.93; mean accuracy reward valid : 56.56\n","saving model at epoch 2\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 96.81, map valid : 98.41\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 96.94, map valid : 98.47\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 96.94, map valid : 98.47\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 97.25, map valid : 98.62\n","saving model at epoch 4\n","\n","DISCRIMINATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","togrep : results : epoch 5 ; mean accuracy valid : 97.19, map valid : 98.59\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 3\n","Learning rate_agent : 0.0009025\n","Learning rate_usr : 0.0009025\n","Agent evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 10.92, precision@10 valid : 70.49\n","User model evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 12.02, precision@10 valid : 62.47, reward_accuracy valid 56.73\n","Interaction evaluation!\n","togrep : results : epoch 3 ; mean accuracy pred valid : 4.55, map pred valid : 12.05; mean accuracy reward valid : 56.73\n","saving model at epoch 3\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 98.12, map valid : 99.06\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 98.25, map valid : 99.12\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.06, map valid : 99.03\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 98.12, map valid : 99.06\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 4\n","Learning rate_agent : 0.000857375\n","Learning rate_usr : 0.000857375\n","Agent evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 10.89, precision@10 valid : 70.61\n","User model evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 12.11, precision@10 valid : 63.13, reward_accuracy valid 56.59\n","Interaction evaluation!\n","togrep : results : epoch 4 ; mean accuracy pred valid : 4.4, map pred valid : 11.96; mean accuracy reward valid : 56.59\n","saving model at epoch 4\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 96.94, map valid : 98.47\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 96.94, map valid : 98.47\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 97.19, map valid : 98.59\n","saving model at epoch 3\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 97.0, map valid : 98.5\n","\n","DISCRIMINATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","togrep : results : epoch 5 ; mean accuracy valid : 96.94, map valid : 98.47\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 5\n","Learning rate_agent : 0.0008145062499999999\n","Learning rate_usr : 0.0008145062499999999\n","Agent evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 10.83, precision@10 valid : 70.81\n","User model evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 12.42, precision@10 valid : 63.77, reward_accuracy valid 56.79\n","Interaction evaluation!\n","togrep : results : epoch 5 ; mean accuracy pred valid : 4.63, map pred valid : 12.24; mean accuracy reward valid : 56.79\n","saving model at epoch 5\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 98.31, map valid : 99.16\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 98.25, map valid : 99.12\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.25, map valid : 99.12\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 6\n","Learning rate_agent : 0.0007737809374999998\n","Learning rate_usr : 0.0007737809374999998\n","Agent evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 10.66, precision@10 valid : 71.04\n","User model evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 12.42, precision@10 valid : 64.09, reward_accuracy valid 56.79\n","Interaction evaluation!\n","togrep : results : epoch 6 ; mean accuracy pred valid : 4.52, map pred valid : 12.24; mean accuracy reward valid : 56.79\n","saving model at epoch 6\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 97.81, map valid : 98.91\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 98.25, map valid : 99.12\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.12, map valid : 99.06\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 98.25, map valid : 99.12\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 7\n","Learning rate_agent : 0.0007350918906249997\n","Learning rate_usr : 0.0007350918906249997\n","Agent evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 10.66, precision@10 valid : 71.1\n","User model evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 12.6, precision@10 valid : 64.78, reward_accuracy valid 56.82\n","Interaction evaluation!\n","togrep : results : epoch 7 ; mean accuracy pred valid : 4.81, map pred valid : 12.41; mean accuracy reward valid : 56.82\n","saving model at epoch 7\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 98.75, map valid : 99.38\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 98.5, map valid : 99.25\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.75, map valid : 99.38\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 8\n","Learning rate_agent : 0.0006983372960937497\n","Learning rate_usr : 0.0006983372960937497\n","Agent evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 10.66, precision@10 valid : 71.24\n","User model evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 12.63, precision@10 valid : 64.67, reward_accuracy valid 56.82\n","Interaction evaluation!\n","togrep : results : epoch 8 ; mean accuracy pred valid : 4.63, map pred valid : 12.36; mean accuracy reward valid : 56.82\n","saving model at epoch 8\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 97.5, map valid : 98.75\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 97.56, map valid : 98.78\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 97.56, map valid : 98.78\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 97.44, map valid : 98.72\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 9\n","Learning rate_agent : 0.0006634204312890621\n","Learning rate_usr : 0.0006634204312890621\n","Agent evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 10.57, precision@10 valid : 71.18\n","User model evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 12.74, precision@10 valid : 64.67, reward_accuracy valid 56.82\n","Interaction evaluation!\n","togrep : results : epoch 9 ; mean accuracy pred valid : 4.75, map pred valid : 12.37; mean accuracy reward valid : 56.82\n","saving model at epoch 9\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 98.38, map valid : 99.19\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 98.06, map valid : 99.03\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.0, map valid : 99.0\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 10\n","Learning rate_agent : 0.000630249409724609\n","Learning rate_usr : 0.000630249409724609\n","Agent evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 10.51, precision@10 valid : 71.18\n","User model evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 12.89, precision@10 valid : 64.87, reward_accuracy valid 56.82\n","Interaction evaluation!\n","togrep : results : epoch 10 ; mean accuracy pred valid : 4.69, map pred valid : 12.42; mean accuracy reward valid : 56.82\n","saving model at epoch 10\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 97.5, map valid : 98.75\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 97.19, map valid : 98.59\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 97.19, map valid : 98.59\n","Testing\n","Agent evaluation!\n","\n","VALIDATION : Epoch 101\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:153: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return np.array(self.itemSeq[batchstart:batchend]), np.array(self.target[batchstart:batchend]), np.array(self.reward[batchstart:batchend]), np.array(self.action[batchstart:batchend])\n","togrep : results : epoch 101 ; accuracy test : 9.88, precision@10 test : 71.19\n","User model evaluation!\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; accuracy test : 13.65, precision@10 test : 63.9, reward_accuracy test 57.16\n","The original reward is:1.7191\n","The optimal reward is:1.7888\n","\n","The original reward is: 1.8911\n","==============================================\n","Training on the epoch:2\n","==============================================\n","100\n","Train seq : 8000\n","Valid seq : 1000\n","Test seq : 1000\n","20\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:215: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  trainSample.genSample_pred(np.array(clicklist)[trainindex].tolist(), np.array(rewardlist)[trainindex].tolist(), np.array(actionlist)[trainindex].tolist(), warm_up, real_num_label, rec_len, add_end = True)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:216: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  validSample.genSample_predtest(np.array(clicklist)[validindex].tolist(),np.array(rewardlist)[validindex].tolist(), np.array(actionlist)[validindex].tolist(), warm_up, real_num_label, add_end = False)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:217: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  testSample.genSample_predtest(np.array(clicklist)[testindex].tolist(),np.array(rewardlist)[testindex].tolist(), np.array(actionlist)[testindex].tolist(), warm_up, real_num_label, add_end = False)\n","\n","--------------------------------------------\n","Pretrain Generator with given recommendation\n","--------------------------------------------\n","\n","GENERATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:153: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return np.array(self.itemSeq[batchstart:batchend]), np.array(self.target[batchstart:batchend]), np.array(self.reward[batchstart:batchend]), np.array(self.action[batchstart:batchend])\n","results : epoch 1 ; mean accuracy pred : 23.77; mean P@10 pred: 54.04; mean accuracy reward: 64.04\n","User model evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 6.76, precision@10 valid : 46.82, reward_accuracy valid 57.52\n","saving model at epoch 1\n","\n","GENERATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","results : epoch 2 ; mean accuracy pred : 24.27; mean P@10 pred: 54.91; mean accuracy reward: 67.62\n","User model evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 7.06, precision@10 valid : 48.12, reward_accuracy valid 58.3\n","saving model at epoch 2\n","\n","GENERATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","results : epoch 3 ; mean accuracy pred : 24.87; mean P@10 pred: 56.51; mean accuracy reward: 68.16\n","User model evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 8.4, precision@10 valid : 50.07, reward_accuracy valid 58.19\n","saving model at epoch 3\n","\n","GENERATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","results : epoch 4 ; mean accuracy pred : 26.17; mean P@10 pred: 59.1; mean accuracy reward: 68.21\n","User model evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 8.98, precision@10 valid : 51.43, reward_accuracy valid 58.19\n","saving model at epoch 4\n","\n","GENERATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","results : epoch 5 ; mean accuracy pred : 26.62; mean P@10 pred: 60.94; mean accuracy reward: 68.28\n","User model evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 9.45, precision@10 valid : 52.79, reward_accuracy valid 58.3\n","saving model at epoch 5\n","\n","GENERATOR TRAINING : Epoch 6\n","Learning rate : 0.0007737809374999998\n","results : epoch 6 ; mean accuracy pred : 26.99; mean P@10 pred: 62.02; mean accuracy reward: 68.31\n","User model evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 9.54, precision@10 valid : 54.18, reward_accuracy valid 58.44\n","saving model at epoch 6\n","\n","GENERATOR TRAINING : Epoch 7\n","Learning rate : 0.0007350918906249997\n","results : epoch 7 ; mean accuracy pred : 27.38; mean P@10 pred: 62.75; mean accuracy reward: 68.35\n","User model evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 9.95, precision@10 valid : 54.88, reward_accuracy valid 58.49\n","saving model at epoch 7\n","\n","GENERATOR TRAINING : Epoch 8\n","Learning rate : 0.0006983372960937497\n","results : epoch 8 ; mean accuracy pred : 27.73; mean P@10 pred: 63.44; mean accuracy reward: 68.36\n","User model evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 10.56, precision@10 valid : 55.32, reward_accuracy valid 58.49\n","saving model at epoch 8\n","\n","GENERATOR TRAINING : Epoch 9\n","Learning rate : 0.0006634204312890621\n","results : epoch 9 ; mean accuracy pred : 28.0; mean P@10 pred: 63.77; mean accuracy reward: 68.37\n","User model evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 10.68, precision@10 valid : 55.49, reward_accuracy valid 58.55\n","saving model at epoch 9\n","\n","GENERATOR TRAINING : Epoch 10\n","Learning rate : 0.000630249409724609\n","results : epoch 10 ; mean accuracy pred : 28.06; mean P@10 pred: 64.3; mean accuracy reward: 68.34\n","User model evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 10.54, precision@10 valid : 55.8, reward_accuracy valid 58.58\n","saving model at epoch 10\n","\n","GENERATOR TRAINING : Epoch 11\n","Learning rate : 0.0005987369392383785\n","results : epoch 11 ; mean accuracy pred : 28.13; mean P@10 pred: 64.66; mean accuracy reward: 68.32\n","User model evaluation!\n","togrep : results : epoch 11 ; accuracy valid : 10.65, precision@10 valid : 56.07, reward_accuracy valid 58.58\n","saving model at epoch 11\n","\n","GENERATOR TRAINING : Epoch 12\n","Learning rate : 0.0005688000922764595\n","results : epoch 12 ; mean accuracy pred : 28.18; mean P@10 pred: 64.98; mean accuracy reward: 68.27\n","User model evaluation!\n","togrep : results : epoch 12 ; accuracy valid : 10.76, precision@10 valid : 56.3, reward_accuracy valid 58.58\n","saving model at epoch 12\n","\n","GENERATOR TRAINING : Epoch 13\n","Learning rate : 0.0005403600876626365\n","results : epoch 13 ; mean accuracy pred : 28.18; mean P@10 pred: 65.11; mean accuracy reward: 68.26\n","User model evaluation!\n","togrep : results : epoch 13 ; accuracy valid : 10.93, precision@10 valid : 56.55, reward_accuracy valid 58.58\n","saving model at epoch 13\n","\n","GENERATOR TRAINING : Epoch 14\n","Learning rate : 0.0005133420832795047\n","results : epoch 14 ; mean accuracy pred : 28.26; mean P@10 pred: 65.2; mean accuracy reward: 68.29\n","User model evaluation!\n","togrep : results : epoch 14 ; accuracy valid : 10.87, precision@10 valid : 56.57, reward_accuracy valid 58.58\n","saving model at epoch 14\n","\n","GENERATOR TRAINING : Epoch 15\n","Learning rate : 0.00048767497911552944\n","results : epoch 15 ; mean accuracy pred : 28.28; mean P@10 pred: 65.42; mean accuracy reward: 68.32\n","User model evaluation!\n","togrep : results : epoch 15 ; accuracy valid : 10.87, precision@10 valid : 56.52, reward_accuracy valid 58.58\n","\n","GENERATOR TRAINING : Epoch 16\n","Learning rate : 0.00046329123015975297\n","results : epoch 16 ; mean accuracy pred : 28.31; mean P@10 pred: 65.5; mean accuracy reward: 68.35\n","User model evaluation!\n","togrep : results : epoch 16 ; accuracy valid : 10.9, precision@10 valid : 56.8, reward_accuracy valid 58.58\n","saving model at epoch 16\n","\n","GENERATOR TRAINING : Epoch 17\n","Learning rate : 0.0004401266686517653\n","results : epoch 17 ; mean accuracy pred : 28.33; mean P@10 pred: 65.65; mean accuracy reward: 68.38\n","User model evaluation!\n","togrep : results : epoch 17 ; accuracy valid : 10.93, precision@10 valid : 56.71, reward_accuracy valid 58.58\n","saving model at epoch 17\n","\n","GENERATOR TRAINING : Epoch 18\n","Learning rate : 0.00041812033521917703\n","results : epoch 18 ; mean accuracy pred : 28.3; mean P@10 pred: 65.88; mean accuracy reward: 68.38\n","User model evaluation!\n","togrep : results : epoch 18 ; accuracy valid : 11.06, precision@10 valid : 57.08, reward_accuracy valid 58.58\n","saving model at epoch 18\n","\n","GENERATOR TRAINING : Epoch 19\n","Learning rate : 0.00039721431845821814\n","results : epoch 19 ; mean accuracy pred : 28.34; mean P@10 pred: 66.11; mean accuracy reward: 68.41\n","User model evaluation!\n","togrep : results : epoch 19 ; accuracy valid : 11.06, precision@10 valid : 57.38, reward_accuracy valid 58.58\n","saving model at epoch 19\n","\n","GENERATOR TRAINING : Epoch 20\n","Learning rate : 0.0003773536025353072\n","results : epoch 20 ; mean accuracy pred : 28.37; mean P@10 pred: 66.3; mean accuracy reward: 68.41\n","User model evaluation!\n","togrep : results : epoch 20 ; accuracy valid : 11.15, precision@10 valid : 57.8, reward_accuracy valid 58.58\n","saving model at epoch 20\n","\n","GENERATOR TRAINING : Epoch 21\n","Learning rate : 0.0003584859224085418\n","results : epoch 21 ; mean accuracy pred : 28.37; mean P@10 pred: 66.51; mean accuracy reward: 68.4\n","User model evaluation!\n","togrep : results : epoch 21 ; accuracy valid : 11.09, precision@10 valid : 58.02, reward_accuracy valid 58.58\n","saving model at epoch 21\n","\n","GENERATOR TRAINING : Epoch 22\n","Learning rate : 0.0003405616262881147\n","results : epoch 22 ; mean accuracy pred : 28.39; mean P@10 pred: 66.76; mean accuracy reward: 68.42\n","User model evaluation!\n","togrep : results : epoch 22 ; accuracy valid : 11.09, precision@10 valid : 58.3, reward_accuracy valid 58.58\n","saving model at epoch 22\n","\n","GENERATOR TRAINING : Epoch 23\n","Learning rate : 0.00032353354497370894\n","results : epoch 23 ; mean accuracy pred : 28.4; mean P@10 pred: 66.97; mean accuracy reward: 68.43\n","User model evaluation!\n","togrep : results : epoch 23 ; accuracy valid : 11.12, precision@10 valid : 58.35, reward_accuracy valid 58.58\n","saving model at epoch 23\n","\n","GENERATOR TRAINING : Epoch 24\n","Learning rate : 0.00030735686772502346\n","results : epoch 24 ; mean accuracy pred : 28.41; mean P@10 pred: 67.15; mean accuracy reward: 68.44\n","User model evaluation!\n","togrep : results : epoch 24 ; accuracy valid : 11.12, precision@10 valid : 58.74, reward_accuracy valid 58.58\n","saving model at epoch 24\n","\n","GENERATOR TRAINING : Epoch 25\n","Learning rate : 0.00029198902433877225\n","results : epoch 25 ; mean accuracy pred : 28.46; mean P@10 pred: 67.33; mean accuracy reward: 68.43\n","User model evaluation!\n","togrep : results : epoch 25 ; accuracy valid : 11.23, precision@10 valid : 58.88, reward_accuracy valid 58.58\n","saving model at epoch 25\n","\n","GENERATOR TRAINING : Epoch 26\n","Learning rate : 0.00027738957312183364\n","results : epoch 26 ; mean accuracy pred : 28.55; mean P@10 pred: 67.56; mean accuracy reward: 68.42\n","User model evaluation!\n","togrep : results : epoch 26 ; accuracy valid : 11.34, precision@10 valid : 58.94, reward_accuracy valid 58.58\n","saving model at epoch 26\n","\n","GENERATOR TRAINING : Epoch 27\n","Learning rate : 0.0002635200944657419\n","results : epoch 27 ; mean accuracy pred : 28.58; mean P@10 pred: 67.78; mean accuracy reward: 68.43\n","User model evaluation!\n","togrep : results : epoch 27 ; accuracy valid : 11.31, precision@10 valid : 59.47, reward_accuracy valid 58.58\n","saving model at epoch 27\n","\n","GENERATOR TRAINING : Epoch 28\n","Learning rate : 0.0002503440897424548\n","results : epoch 28 ; mean accuracy pred : 28.58; mean P@10 pred: 67.97; mean accuracy reward: 68.43\n","User model evaluation!\n","togrep : results : epoch 28 ; accuracy valid : 11.29, precision@10 valid : 59.52, reward_accuracy valid 58.58\n","saving model at epoch 28\n","\n","GENERATOR TRAINING : Epoch 29\n","Learning rate : 0.00023782688525533205\n","results : epoch 29 ; mean accuracy pred : 28.58; mean P@10 pred: 68.21; mean accuracy reward: 68.43\n","User model evaluation!\n","togrep : results : epoch 29 ; accuracy valid : 11.37, precision@10 valid : 59.69, reward_accuracy valid 58.47\n","saving model at epoch 29\n","\n","GENERATOR TRAINING : Epoch 30\n","Learning rate : 0.00022593554099256544\n","results : epoch 30 ; mean accuracy pred : 28.57; mean P@10 pred: 68.31; mean accuracy reward: 68.44\n","User model evaluation!\n","togrep : results : epoch 30 ; accuracy valid : 11.34, precision@10 valid : 60.13, reward_accuracy valid 58.47\n","saving model at epoch 30\n","\n","GENERATOR TRAINING : Epoch 31\n","Learning rate : 0.00021463876394293716\n","results : epoch 31 ; mean accuracy pred : 28.58; mean P@10 pred: 68.47; mean accuracy reward: 68.45\n","User model evaluation!\n","togrep : results : epoch 31 ; accuracy valid : 11.34, precision@10 valid : 60.49, reward_accuracy valid 58.47\n","saving model at epoch 31\n","\n","GENERATOR TRAINING : Epoch 32\n","Learning rate : 0.0002039068257457903\n","results : epoch 32 ; mean accuracy pred : 28.57; mean P@10 pred: 68.57; mean accuracy reward: 68.46\n","User model evaluation!\n","togrep : results : epoch 32 ; accuracy valid : 11.34, precision@10 valid : 60.63, reward_accuracy valid 58.47\n","saving model at epoch 32\n","\n","GENERATOR TRAINING : Epoch 33\n","Learning rate : 0.00019371148445850077\n","results : epoch 33 ; mean accuracy pred : 28.57; mean P@10 pred: 68.75; mean accuracy reward: 68.45\n","User model evaluation!\n","togrep : results : epoch 33 ; accuracy valid : 11.48, precision@10 valid : 60.61, reward_accuracy valid 58.47\n","saving model at epoch 33\n","\n","GENERATOR TRAINING : Epoch 34\n","Learning rate : 0.00018402591023557573\n","results : epoch 34 ; mean accuracy pred : 28.61; mean P@10 pred: 68.76; mean accuracy reward: 68.45\n","User model evaluation!\n","togrep : results : epoch 34 ; accuracy valid : 11.45, precision@10 valid : 60.58, reward_accuracy valid 58.49\n","\n","GENERATOR TRAINING : Epoch 35\n","Learning rate : 0.00017482461472379692\n","results : epoch 35 ; mean accuracy pred : 28.61; mean P@10 pred: 68.89; mean accuracy reward: 68.46\n","User model evaluation!\n","togrep : results : epoch 35 ; accuracy valid : 11.48, precision@10 valid : 60.91, reward_accuracy valid 58.49\n","saving model at epoch 35\n","\n","GENERATOR TRAINING : Epoch 36\n","Learning rate : 0.00016608338398760707\n","results : epoch 36 ; mean accuracy pred : 28.64; mean P@10 pred: 68.97; mean accuracy reward: 68.46\n","User model evaluation!\n","togrep : results : epoch 36 ; accuracy valid : 11.43, precision@10 valid : 61.02, reward_accuracy valid 58.49\n","saving model at epoch 36\n","\n","GENERATOR TRAINING : Epoch 37\n","Learning rate : 0.0001577792147882267\n","results : epoch 37 ; mean accuracy pred : 28.64; mean P@10 pred: 69.09; mean accuracy reward: 68.46\n","User model evaluation!\n","togrep : results : epoch 37 ; accuracy valid : 11.4, precision@10 valid : 61.25, reward_accuracy valid 58.52\n","saving model at epoch 37\n","\n","GENERATOR TRAINING : Epoch 38\n","Learning rate : 0.00014989025404881537\n","results : epoch 38 ; mean accuracy pred : 28.67; mean P@10 pred: 69.11; mean accuracy reward: 68.47\n","User model evaluation!\n","togrep : results : epoch 38 ; accuracy valid : 11.45, precision@10 valid : 61.36, reward_accuracy valid 58.52\n","saving model at epoch 38\n","\n","GENERATOR TRAINING : Epoch 39\n","Learning rate : 0.00014239574134637458\n","results : epoch 39 ; mean accuracy pred : 28.66; mean P@10 pred: 69.26; mean accuracy reward: 68.47\n","User model evaluation!\n","togrep : results : epoch 39 ; accuracy valid : 11.45, precision@10 valid : 61.27, reward_accuracy valid 58.49\n","\n","GENERATOR TRAINING : Epoch 40\n","Learning rate : 0.00013527595427905584\n","results : epoch 40 ; mean accuracy pred : 28.68; mean P@10 pred: 69.31; mean accuracy reward: 68.47\n","User model evaluation!\n","togrep : results : epoch 40 ; accuracy valid : 11.57, precision@10 valid : 61.33, reward_accuracy valid 58.49\n","saving model at epoch 40\n","\n","GENERATOR TRAINING : Epoch 41\n","Learning rate : 0.00012851215656510304\n","results : epoch 41 ; mean accuracy pred : 28.72; mean P@10 pred: 69.38; mean accuracy reward: 68.48\n","User model evaluation!\n","togrep : results : epoch 41 ; accuracy valid : 11.62, precision@10 valid : 61.47, reward_accuracy valid 58.49\n","saving model at epoch 41\n","\n","GENERATOR TRAINING : Epoch 42\n","Learning rate : 0.00012208654873684788\n","results : epoch 42 ; mean accuracy pred : 28.76; mean P@10 pred: 69.42; mean accuracy reward: 68.48\n","User model evaluation!\n","togrep : results : epoch 42 ; accuracy valid : 11.62, precision@10 valid : 61.55, reward_accuracy valid 58.49\n","saving model at epoch 42\n","\n","GENERATOR TRAINING : Epoch 43\n","Learning rate : 0.00011598222130000548\n","results : epoch 43 ; mean accuracy pred : 28.78; mean P@10 pred: 69.52; mean accuracy reward: 68.47\n","User model evaluation!\n","togrep : results : epoch 43 ; accuracy valid : 11.73, precision@10 valid : 61.63, reward_accuracy valid 58.49\n","saving model at epoch 43\n","\n","GENERATOR TRAINING : Epoch 44\n","Learning rate : 0.00011018311023500519\n","results : epoch 44 ; mean accuracy pred : 28.77; mean P@10 pred: 69.54; mean accuracy reward: 68.47\n","User model evaluation!\n","togrep : results : epoch 44 ; accuracy valid : 11.73, precision@10 valid : 61.75, reward_accuracy valid 58.49\n","saving model at epoch 44\n","\n","GENERATOR TRAINING : Epoch 45\n","Learning rate : 0.00010467395472325493\n","results : epoch 45 ; mean accuracy pred : 28.77; mean P@10 pred: 69.59; mean accuracy reward: 68.47\n","User model evaluation!\n","togrep : results : epoch 45 ; accuracy valid : 11.76, precision@10 valid : 61.77, reward_accuracy valid 58.49\n","saving model at epoch 45\n","\n","GENERATOR TRAINING : Epoch 46\n","Learning rate : 9.944025698709218e-05\n","results : epoch 46 ; mean accuracy pred : 28.77; mean P@10 pred: 69.64; mean accuracy reward: 68.49\n","User model evaluation!\n","togrep : results : epoch 46 ; accuracy valid : 11.76, precision@10 valid : 61.86, reward_accuracy valid 58.49\n","saving model at epoch 46\n","\n","GENERATOR TRAINING : Epoch 47\n","Learning rate : 9.446824413773756e-05\n","results : epoch 47 ; mean accuracy pred : 28.77; mean P@10 pred: 69.7; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 47 ; accuracy valid : 11.79, precision@10 valid : 61.86, reward_accuracy valid 58.49\n","saving model at epoch 47\n","\n","GENERATOR TRAINING : Epoch 48\n","Learning rate : 8.974483193085068e-05\n","results : epoch 48 ; mean accuracy pred : 28.77; mean P@10 pred: 69.74; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 48 ; accuracy valid : 11.79, precision@10 valid : 61.88, reward_accuracy valid 58.49\n","saving model at epoch 48\n","\n","GENERATOR TRAINING : Epoch 49\n","Learning rate : 8.525759033430814e-05\n","results : epoch 49 ; mean accuracy pred : 28.76; mean P@10 pred: 69.81; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 49 ; accuracy valid : 11.79, precision@10 valid : 61.91, reward_accuracy valid 58.49\n","saving model at epoch 49\n","\n","GENERATOR TRAINING : Epoch 50\n","Learning rate : 8.099471081759274e-05\n","results : epoch 50 ; mean accuracy pred : 28.76; mean P@10 pred: 69.92; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 50 ; accuracy valid : 11.76, precision@10 valid : 62.02, reward_accuracy valid 58.49\n","saving model at epoch 50\n","\n","GENERATOR TRAINING : Epoch 51\n","Learning rate : 7.69449752767131e-05\n","results : epoch 51 ; mean accuracy pred : 28.78; mean P@10 pred: 69.93; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 51 ; accuracy valid : 11.9, precision@10 valid : 62.11, reward_accuracy valid 58.49\n","saving model at epoch 51\n","\n","GENERATOR TRAINING : Epoch 52\n","Learning rate : 7.309772651287744e-05\n","results : epoch 52 ; mean accuracy pred : 28.79; mean P@10 pred: 69.95; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 52 ; accuracy valid : 11.93, precision@10 valid : 62.22, reward_accuracy valid 58.49\n","saving model at epoch 52\n","\n","GENERATOR TRAINING : Epoch 53\n","Learning rate : 6.944284018723356e-05\n","results : epoch 53 ; mean accuracy pred : 28.79; mean P@10 pred: 69.94; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 53 ; accuracy valid : 11.95, precision@10 valid : 62.19, reward_accuracy valid 58.49\n","saving model at epoch 53\n","\n","GENERATOR TRAINING : Epoch 54\n","Learning rate : 6.597069817787189e-05\n","results : epoch 54 ; mean accuracy pred : 28.79; mean P@10 pred: 69.94; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 54 ; accuracy valid : 11.95, precision@10 valid : 62.22, reward_accuracy valid 58.49\n","saving model at epoch 54\n","\n","GENERATOR TRAINING : Epoch 55\n","Learning rate : 6.267216326897829e-05\n","results : epoch 55 ; mean accuracy pred : 28.79; mean P@10 pred: 69.99; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 55 ; accuracy valid : 11.95, precision@10 valid : 62.3, reward_accuracy valid 58.49\n","saving model at epoch 55\n","\n","GENERATOR TRAINING : Epoch 56\n","Learning rate : 5.953855510552937e-05\n","results : epoch 56 ; mean accuracy pred : 28.79; mean P@10 pred: 70.01; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 56 ; accuracy valid : 11.95, precision@10 valid : 62.27, reward_accuracy valid 58.49\n","\n","GENERATOR TRAINING : Epoch 57\n","Learning rate : 5.65616273502529e-05\n","results : epoch 57 ; mean accuracy pred : 28.79; mean P@10 pred: 70.04; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 57 ; accuracy valid : 11.98, precision@10 valid : 62.27, reward_accuracy valid 58.49\n","saving model at epoch 57\n","\n","GENERATOR TRAINING : Epoch 58\n","Learning rate : 5.373354598274025e-05\n","results : epoch 58 ; mean accuracy pred : 28.79; mean P@10 pred: 70.14; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 58 ; accuracy valid : 11.98, precision@10 valid : 62.36, reward_accuracy valid 58.49\n","saving model at epoch 58\n","\n","GENERATOR TRAINING : Epoch 59\n","Learning rate : 5.104686868360323e-05\n","results : epoch 59 ; mean accuracy pred : 28.78; mean P@10 pred: 70.18; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 59 ; accuracy valid : 11.98, precision@10 valid : 62.36, reward_accuracy valid 58.49\n","\n","GENERATOR TRAINING : Epoch 60\n","Learning rate : 4.849452524942307e-05\n","results : epoch 60 ; mean accuracy pred : 28.79; mean P@10 pred: 70.18; mean accuracy reward: 68.49\n","User model evaluation!\n","togrep : results : epoch 60 ; accuracy valid : 12.09, precision@10 valid : 62.39, reward_accuracy valid 58.49\n","saving model at epoch 60\n","\n","GENERATOR TRAINING : Epoch 61\n","Learning rate : 4.606979898695191e-05\n","results : epoch 61 ; mean accuracy pred : 28.8; mean P@10 pred: 70.2; mean accuracy reward: 68.49\n","User model evaluation!\n","togrep : results : epoch 61 ; accuracy valid : 12.09, precision@10 valid : 62.39, reward_accuracy valid 58.49\n","\n","GENERATOR TRAINING : Epoch 62\n","Learning rate : 4.376630903760431e-05\n","results : epoch 62 ; mean accuracy pred : 28.8; mean P@10 pred: 70.2; mean accuracy reward: 68.49\n","User model evaluation!\n","togrep : results : epoch 62 ; accuracy valid : 12.12, precision@10 valid : 62.47, reward_accuracy valid 58.49\n","saving model at epoch 62\n","\n","GENERATOR TRAINING : Epoch 63\n","Learning rate : 4.157799358572409e-05\n","results : epoch 63 ; mean accuracy pred : 28.8; mean P@10 pred: 70.25; mean accuracy reward: 68.49\n","User model evaluation!\n","togrep : results : epoch 63 ; accuracy valid : 12.12, precision@10 valid : 62.47, reward_accuracy valid 58.49\n","\n","GENERATOR TRAINING : Epoch 64\n","Learning rate : 3.9499093906437885e-05\n","results : epoch 64 ; mean accuracy pred : 28.8; mean P@10 pred: 70.25; mean accuracy reward: 68.49\n","User model evaluation!\n","togrep : results : epoch 64 ; accuracy valid : 12.15, precision@10 valid : 62.47, reward_accuracy valid 58.49\n","saving model at epoch 64\n","\n","GENERATOR TRAINING : Epoch 65\n","Learning rate : 3.752413921111599e-05\n","results : epoch 65 ; mean accuracy pred : 28.79; mean P@10 pred: 70.28; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 65 ; accuracy valid : 12.15, precision@10 valid : 62.61, reward_accuracy valid 58.49\n","saving model at epoch 65\n","\n","GENERATOR TRAINING : Epoch 66\n","Learning rate : 3.564793225056019e-05\n","results : epoch 66 ; mean accuracy pred : 28.79; mean P@10 pred: 70.28; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 66 ; accuracy valid : 12.18, precision@10 valid : 62.66, reward_accuracy valid 58.49\n","saving model at epoch 66\n","\n","GENERATOR TRAINING : Epoch 67\n","Learning rate : 3.3865535638032174e-05\n","results : epoch 67 ; mean accuracy pred : 28.79; mean P@10 pred: 70.29; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 67 ; accuracy valid : 12.18, precision@10 valid : 62.61, reward_accuracy valid 58.49\n","\n","GENERATOR TRAINING : Epoch 68\n","Learning rate : 3.2172258856130564e-05\n","results : epoch 68 ; mean accuracy pred : 28.79; mean P@10 pred: 70.31; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 68 ; accuracy valid : 12.18, precision@10 valid : 62.77, reward_accuracy valid 58.49\n","saving model at epoch 68\n","\n","GENERATOR TRAINING : Epoch 69\n","Learning rate : 3.056364591332403e-05\n","results : epoch 69 ; mean accuracy pred : 28.79; mean P@10 pred: 70.31; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 69 ; accuracy valid : 12.18, precision@10 valid : 62.77, reward_accuracy valid 58.49\n","\n","GENERATOR TRAINING : Epoch 70\n","Learning rate : 2.903546361765783e-05\n","results : epoch 70 ; mean accuracy pred : 28.79; mean P@10 pred: 70.31; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 70 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","saving model at epoch 70\n","\n","GENERATOR TRAINING : Epoch 71\n","Learning rate : 2.758369043677494e-05\n","results : epoch 71 ; mean accuracy pred : 28.79; mean P@10 pred: 70.31; mean accuracy reward: 68.49\n","User model evaluation!\n","togrep : results : epoch 71 ; accuracy valid : 12.15, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","\n","GENERATOR TRAINING : Epoch 72\n","Learning rate : 2.620450591493619e-05\n","results : epoch 72 ; mean accuracy pred : 28.78; mean P@10 pred: 70.31; mean accuracy reward: 68.5\n","User model evaluation!\n","togrep : results : epoch 72 ; accuracy valid : 12.15, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Testing\n","User model evaluation!\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; accuracy test : 12.06, precision@10 test : 63.04, reward_accuracy test 57.96\n","\n","--------------------------------------------\n","Pretrain by the adversarial training\n","---------------------------------------------\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:220: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  trainSample.genSample_generator(np.array(clicklist)[trainindex].tolist(), np.array(rewardlist)[trainindex].tolist(), np.array(actionlist)[trainindex].tolist(), real_num_label, rec_len, add_end = True)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:221: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  validSample.genSample_predtest(np.array(clicklist)[validindex].tolist(),np.array(rewardlist)[validindex].tolist(), np.array(actionlist)[validindex].tolist(), warm_up, real_num_label, add_end = False)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:222: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  testSample.genSample_predtest(np.array(clicklist)[testindex].tolist(),np.array(rewardlist)[testindex].tolist(), np.array(actionlist)[testindex].tolist(), warm_up, real_num_label, add_end = False)\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 1\n","Learning rate_agent : 0.001\n","Learning rate_usr : 0.001\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:158: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  self.itemSeq = np.array(origin.itemSeq)[index[:subnum]].tolist()\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  self.reward = np.array(origin.reward)[index[:subnum]].tolist()\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  self.action = np.array(origin.action)[index[:subnum]].tolist()\n","Agent evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 4.67, precision@10 valid : 52.6\n","User model evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 1 ; mean accuracy pred valid : 1.36, map pred valid : 6.66; mean accuracy reward valid : 58.49\n","saving model at epoch 1\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 2\n","Learning rate_agent : 0.00095\n","Learning rate_usr : 0.00095\n","Agent evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 5.67, precision@10 valid : 56.16\n","User model evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 2 ; mean accuracy pred valid : 1.47, map pred valid : 7.24; mean accuracy reward valid : 58.49\n","saving model at epoch 2\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 3\n","Learning rate_agent : 0.0009025\n","Learning rate_usr : 0.0009025\n","Agent evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 6.87, precision@10 valid : 57.46\n","User model evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 3 ; mean accuracy pred valid : 1.47, map pred valid : 7.41; mean accuracy reward valid : 58.49\n","saving model at epoch 3\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 4\n","Learning rate_agent : 0.000857375\n","Learning rate_usr : 0.000857375\n","Agent evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 7.65, precision@10 valid : 58.55\n","User model evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 4 ; mean accuracy pred valid : 1.47, map pred valid : 7.51; mean accuracy reward valid : 58.49\n","saving model at epoch 4\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 5\n","Learning rate_agent : 0.0008145062499999999\n","Learning rate_usr : 0.0008145062499999999\n","Agent evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 8.09, precision@10 valid : 58.69\n","User model evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 5 ; mean accuracy pred valid : 1.5, map pred valid : 7.54; mean accuracy reward valid : 58.49\n","saving model at epoch 5\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 6\n","Learning rate_agent : 0.0007737809374999998\n","Learning rate_usr : 0.0007737809374999998\n","Agent evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 8.23, precision@10 valid : 59.13\n","User model evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 6 ; mean accuracy pred valid : 1.5, map pred valid : 7.57; mean accuracy reward valid : 58.49\n","saving model at epoch 6\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 7\n","Learning rate_agent : 0.0007350918906249997\n","Learning rate_usr : 0.0007350918906249997\n","Agent evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 8.31, precision@10 valid : 59.38\n","User model evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 7 ; mean accuracy pred valid : 1.53, map pred valid : 7.62; mean accuracy reward valid : 58.49\n","saving model at epoch 7\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 8\n","Learning rate_agent : 0.0006983372960937497\n","Learning rate_usr : 0.0006983372960937497\n","Agent evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 8.23, precision@10 valid : 59.77\n","User model evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 8 ; mean accuracy pred valid : 1.53, map pred valid : 7.65; mean accuracy reward valid : 58.49\n","saving model at epoch 8\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 9\n","Learning rate_agent : 0.0006634204312890621\n","Learning rate_usr : 0.0006634204312890621\n","Agent evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 8.28, precision@10 valid : 60.22\n","User model evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 9 ; mean accuracy pred valid : 1.56, map pred valid : 7.69; mean accuracy reward valid : 58.49\n","saving model at epoch 9\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 10\n","Learning rate_agent : 0.000630249409724609\n","Learning rate_usr : 0.000630249409724609\n","Agent evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 8.53, precision@10 valid : 60.52\n","User model evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 10 ; mean accuracy pred valid : 1.58, map pred valid : 7.54; mean accuracy reward valid : 58.49\n","saving model at epoch 10\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 11\n","Learning rate_agent : 0.0005987369392383785\n","Learning rate_usr : 0.0005987369392383785\n","Agent evaluation!\n","togrep : results : epoch 11 ; accuracy valid : 8.56, precision@10 valid : 60.69\n","User model evaluation!\n","togrep : results : epoch 11 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 11 ; mean accuracy pred valid : 1.64, map pred valid : 7.58; mean accuracy reward valid : 58.49\n","saving model at epoch 11\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 12\n","Learning rate_agent : 0.0005688000922764595\n","Learning rate_usr : 0.0005688000922764595\n","Agent evaluation!\n","togrep : results : epoch 12 ; accuracy valid : 8.79, precision@10 valid : 61.02\n","User model evaluation!\n","togrep : results : epoch 12 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 12 ; mean accuracy pred valid : 1.72, map pred valid : 7.63; mean accuracy reward valid : 58.49\n","saving model at epoch 12\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 13\n","Learning rate_agent : 0.0005403600876626365\n","Learning rate_usr : 0.0005403600876626365\n","Agent evaluation!\n","togrep : results : epoch 13 ; accuracy valid : 8.87, precision@10 valid : 61.25\n","User model evaluation!\n","togrep : results : epoch 13 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 13 ; mean accuracy pred valid : 1.7, map pred valid : 7.63; mean accuracy reward valid : 58.49\n","saving model at epoch 13\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 14\n","Learning rate_agent : 0.0005133420832795047\n","Learning rate_usr : 0.0005133420832795047\n","Agent evaluation!\n","togrep : results : epoch 14 ; accuracy valid : 8.56, precision@10 valid : 61.41\n","User model evaluation!\n","togrep : results : epoch 14 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 14 ; mean accuracy pred valid : 1.7, map pred valid : 7.67; mean accuracy reward valid : 58.49\n","saving model at epoch 14\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 15\n","Learning rate_agent : 0.00048767497911552944\n","Learning rate_usr : 0.00048767497911552944\n","Agent evaluation!\n","togrep : results : epoch 15 ; accuracy valid : 8.62, precision@10 valid : 61.69\n","User model evaluation!\n","togrep : results : epoch 15 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 15 ; mean accuracy pred valid : 1.7, map pred valid : 7.69; mean accuracy reward valid : 58.49\n","saving model at epoch 15\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 16\n","Learning rate_agent : 0.00046329123015975297\n","Learning rate_usr : 0.00046329123015975297\n","Agent evaluation!\n","togrep : results : epoch 16 ; accuracy valid : 8.53, precision@10 valid : 61.66\n","User model evaluation!\n","togrep : results : epoch 16 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 16 ; mean accuracy pred valid : 1.7, map pred valid : 7.69; mean accuracy reward valid : 58.49\n","saving model at epoch 16\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 17\n","Learning rate_agent : 0.0004401266686517653\n","Learning rate_usr : 0.0004401266686517653\n","Agent evaluation!\n","togrep : results : epoch 17 ; accuracy valid : 8.84, precision@10 valid : 61.97\n","User model evaluation!\n","togrep : results : epoch 17 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 17 ; mean accuracy pred valid : 1.7, map pred valid : 7.67; mean accuracy reward valid : 58.49\n","saving model at epoch 17\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 18\n","Learning rate_agent : 0.00041812033521917703\n","Learning rate_usr : 0.00041812033521917703\n","Agent evaluation!\n","togrep : results : epoch 18 ; accuracy valid : 9.01, precision@10 valid : 61.97\n","User model evaluation!\n","togrep : results : epoch 18 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 18 ; mean accuracy pred valid : 1.7, map pred valid : 7.67; mean accuracy reward valid : 58.49\n","saving model at epoch 18\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 19\n","Learning rate_agent : 0.00039721431845821814\n","Learning rate_usr : 0.00039721431845821814\n","Agent evaluation!\n","togrep : results : epoch 19 ; accuracy valid : 9.01, precision@10 valid : 62.05\n","User model evaluation!\n","togrep : results : epoch 19 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 19 ; mean accuracy pred valid : 1.7, map pred valid : 7.67; mean accuracy reward valid : 58.49\n","saving model at epoch 19\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Pretraining\n","\n","TRAINING : Epoch 20\n","Learning rate_agent : 0.0003773536025353072\n","Learning rate_usr : 0.0003773536025353072\n","Agent evaluation!\n","togrep : results : epoch 20 ; accuracy valid : 9.06, precision@10 valid : 62.0\n","User model evaluation!\n","togrep : results : epoch 20 ; accuracy valid : 12.18, precision@10 valid : 62.8, reward_accuracy valid 58.49\n","Interaction evaluation!\n","togrep : results : epoch 20 ; mean accuracy pred valid : 1.7, map pred valid : 7.68; mean accuracy reward valid : 58.49\n","saving model at epoch 20\n","Testing\n","Agent evaluation!\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; accuracy test : 8.14, precision@10 test : 61.94\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:225: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  trainSample.genSample_generator(np.array(clicklist)[trainindex].tolist(), np.array(rewardlist)[trainindex].tolist(), np.array(actionlist)[trainindex].tolist(), real_num_label, False)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:226: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  validSample.genSample_generator(np.array(clicklist)[validindex].tolist(), np.array(rewardlist)[validindex].tolist(), np.array(actionlist)[validindex].tolist(), real_num_label, False)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  testSample.genSample_generator(np.array(clicklist)[testindex].tolist(), np.array(rewardlist)[testindex].tolist(), np.array(actionlist)[testindex].tolist(), real_num_label, False)\n","Generate sample : 8000\n","\n","--------------------------------------------\n","Pretrain the Discriminator\n","--------------------------------------------\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:230: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  trainSample.genSample_discriminator(np.array(clicklist)[trainindex].tolist(), np.array(rewardlist)[trainindex].tolist(), np.array(actionlist)[trainindex].tolist(), np.array(targetlist)[trainindex].tolist(), real_num_label)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:231: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  validSample.genSample_discriminator(np.array(clicklist)[validindex].tolist(), np.array(rewardlist)[validindex].tolist(), np.array(actionlist)[validindex].tolist(), np.array(targetlist)[validindex].tolist(), real_num_label)\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:232: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  testSample.genSample_discriminator(np.array(clicklist)[testindex].tolist(), np.array(rewardlist)[testindex].tolist(), np.array(actionlist)[testindex].tolist(), np.array(targetlist)[testindex].tolist(), real_num_label)\n","Train sample : 12800\n","Valid sample : 1600\n","Test sample : 1600\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 95.75, map valid : 97.88\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 96.62, map valid : 98.31\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 96.81, map valid : 98.41\n","saving model at epoch 3\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 97.06, map valid : 98.53\n","saving model at epoch 4\n","\n","DISCRIMINATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","togrep : results : epoch 5 ; mean accuracy valid : 97.5, map valid : 98.75\n","saving model at epoch 5\n","Testing\n","Discriminator evaluation!\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; mean accuracy test : 98.0, map test : 99.0\n","\n","--------------------------------------------\n","Adversarial Training\n","--------------------------------------------\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; accuracy test : 8.14, precision@10 test : 61.94\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 1\n","Learning rate_agent : 0.001\n","Learning rate_usr : 0.001\n","Agent evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 9.17, precision@10 valid : 62.91\n","User model evaluation!\n","togrep : results : epoch 1 ; accuracy valid : 11.7, precision@10 valid : 65.69, reward_accuracy valid 57.99\n","Interaction evaluation!\n","togrep : results : epoch 1 ; mean accuracy pred valid : 1.5, map pred valid : 7.48; mean accuracy reward valid : 57.99\n","saving model at epoch 1\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 92.38, map valid : 96.19\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 96.38, map valid : 98.19\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 97.31, map valid : 98.66\n","saving model at epoch 3\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 97.56, map valid : 98.78\n","saving model at epoch 4\n","\n","DISCRIMINATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","togrep : results : epoch 5 ; mean accuracy valid : 97.88, map valid : 98.94\n","saving model at epoch 5\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 2\n","Learning rate_agent : 0.00095\n","Learning rate_usr : 0.00095\n","Agent evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 9.51, precision@10 valid : 62.86\n","User model evaluation!\n","togrep : results : epoch 2 ; accuracy valid : 12.84, precision@10 valid : 67.08, reward_accuracy valid 58.35\n","Interaction evaluation!\n","togrep : results : epoch 2 ; mean accuracy pred valid : 1.7, map pred valid : 7.86; mean accuracy reward valid : 58.35\n","saving model at epoch 2\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 98.0, map valid : 99.0\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 97.75, map valid : 98.88\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.06, map valid : 99.03\n","saving model at epoch 3\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 98.06, map valid : 99.03\n","\n","DISCRIMINATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","togrep : results : epoch 5 ; mean accuracy valid : 98.0, map valid : 99.0\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 3\n","Learning rate_agent : 0.0009025\n","Learning rate_usr : 0.0009025\n","Agent evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 9.73, precision@10 valid : 63.11\n","User model evaluation!\n","togrep : results : epoch 3 ; accuracy valid : 13.48, precision@10 valid : 67.89, reward_accuracy valid 58.35\n","Interaction evaluation!\n","togrep : results : epoch 3 ; mean accuracy pred valid : 2.0, map pred valid : 8.43; mean accuracy reward valid : 58.35\n","saving model at epoch 3\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 98.31, map valid : 99.16\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 98.25, map valid : 99.12\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.38, map valid : 99.19\n","saving model at epoch 3\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 98.38, map valid : 99.19\n","\n","DISCRIMINATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","togrep : results : epoch 5 ; mean accuracy valid : 98.06, map valid : 99.03\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 4\n","Learning rate_agent : 0.000857375\n","Learning rate_usr : 0.000857375\n","Agent evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 9.56, precision@10 valid : 63.22\n","User model evaluation!\n","togrep : results : epoch 4 ; accuracy valid : 13.73, precision@10 valid : 68.97, reward_accuracy valid 58.35\n","Interaction evaluation!\n","togrep : results : epoch 4 ; mean accuracy pred valid : 1.78, map pred valid : 8.38; mean accuracy reward valid : 58.35\n","saving model at epoch 4\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 98.81, map valid : 99.41\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 98.88, map valid : 99.44\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.75, map valid : 99.38\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 98.94, map valid : 99.47\n","saving model at epoch 4\n","\n","DISCRIMINATOR TRAINING : Epoch 5\n","Learning rate : 0.0008145062499999999\n","togrep : results : epoch 5 ; mean accuracy valid : 98.94, map valid : 99.47\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 5\n","Learning rate_agent : 0.0008145062499999999\n","Learning rate_usr : 0.0008145062499999999\n","Agent evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 9.56, precision@10 valid : 63.5\n","User model evaluation!\n","togrep : results : epoch 5 ; accuracy valid : 13.71, precision@10 valid : 69.61, reward_accuracy valid 58.35\n","Interaction evaluation!\n","togrep : results : epoch 5 ; mean accuracy pred valid : 1.78, map pred valid : 8.29; mean accuracy reward valid : 58.35\n","saving model at epoch 5\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 98.62, map valid : 99.31\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 98.62, map valid : 99.31\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.44, map valid : 99.22\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 6\n","Learning rate_agent : 0.0007737809374999998\n","Learning rate_usr : 0.0007737809374999998\n","Agent evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 9.31, precision@10 valid : 63.58\n","User model evaluation!\n","togrep : results : epoch 6 ; accuracy valid : 14.32, precision@10 valid : 69.84, reward_accuracy valid 58.35\n","Interaction evaluation!\n","togrep : results : epoch 6 ; mean accuracy pred valid : 1.83, map pred valid : 8.52; mean accuracy reward valid : 58.35\n","saving model at epoch 6\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 98.69, map valid : 99.34\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 98.69, map valid : 99.34\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.56, map valid : 99.28\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 7\n","Learning rate_agent : 0.0007350918906249997\n","Learning rate_usr : 0.0007350918906249997\n","Agent evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 9.4, precision@10 valid : 63.83\n","User model evaluation!\n","togrep : results : epoch 7 ; accuracy valid : 14.21, precision@10 valid : 70.0, reward_accuracy valid 58.35\n","Interaction evaluation!\n","togrep : results : epoch 7 ; mean accuracy pred valid : 2.2, map pred valid : 8.75; mean accuracy reward valid : 58.35\n","saving model at epoch 7\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 99.06, map valid : 99.53\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 99.19, map valid : 99.59\n","saving model at epoch 2\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 99.12, map valid : 99.56\n","\n","DISCRIMINATOR TRAINING : Epoch 4\n","Learning rate : 0.000857375\n","togrep : results : epoch 4 ; mean accuracy valid : 99.06, map valid : 99.53\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 8\n","Learning rate_agent : 0.0006983372960937497\n","Learning rate_usr : 0.0006983372960937497\n","Agent evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 9.06, precision@10 valid : 64.03\n","User model evaluation!\n","togrep : results : epoch 8 ; accuracy valid : 14.23, precision@10 valid : 69.61, reward_accuracy valid 58.35\n","Interaction evaluation!\n","togrep : results : epoch 8 ; mean accuracy pred valid : 2.5, map pred valid : 8.96; mean accuracy reward valid : 58.35\n","saving model at epoch 8\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 98.75, map valid : 99.38\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 98.69, map valid : 99.34\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.69, map valid : 99.34\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 9\n","Learning rate_agent : 0.0006634204312890621\n","Learning rate_usr : 0.0006634204312890621\n","Agent evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 9.06, precision@10 valid : 64.14\n","User model evaluation!\n","togrep : results : epoch 9 ; accuracy valid : 14.43, precision@10 valid : 70.11, reward_accuracy valid 58.35\n","Interaction evaluation!\n","togrep : results : epoch 9 ; mean accuracy pred valid : 2.31, map pred valid : 8.84; mean accuracy reward valid : 58.35\n","saving model at epoch 9\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 98.31, map valid : 99.16\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 98.31, map valid : 99.16\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.0, map valid : 99.0\n","\n","Adversarial Policy Gradient Training!\n","G-step\n","For Policy Gradient Update\n","\n","TRAINING : Epoch 10\n","Learning rate_agent : 0.000630249409724609\n","Learning rate_usr : 0.000630249409724609\n","Agent evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 9.04, precision@10 valid : 64.53\n","User model evaluation!\n","togrep : results : epoch 10 ; accuracy valid : 14.18, precision@10 valid : 70.09, reward_accuracy valid 58.22\n","Interaction evaluation!\n","togrep : results : epoch 10 ; mean accuracy pred valid : 2.56, map pred valid : 8.91; mean accuracy reward valid : 58.22\n","saving model at epoch 10\n","\n","D-step\n","\n","DISCRIMINATOR TRAINING : Epoch 1\n","Learning rate : 0.001\n","togrep : results : epoch 1 ; mean accuracy valid : 98.44, map valid : 99.22\n","saving model at epoch 1\n","\n","DISCRIMINATOR TRAINING : Epoch 2\n","Learning rate : 0.00095\n","togrep : results : epoch 2 ; mean accuracy valid : 98.25, map valid : 99.12\n","\n","DISCRIMINATOR TRAINING : Epoch 3\n","Learning rate : 0.0009025\n","togrep : results : epoch 3 ; mean accuracy valid : 98.12, map valid : 99.06\n","Testing\n","Agent evaluation!\n","\n","VALIDATION : Epoch 101\n","/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN/data.py:153: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return np.array(self.itemSeq[batchstart:batchend]), np.array(self.target[batchstart:batchend]), np.array(self.reward[batchstart:batchend]), np.array(self.action[batchstart:batchend])\n","togrep : results : epoch 101 ; accuracy test : 8.69, precision@10 test : 64.06\n","User model evaluation!\n","\n","VALIDATION : Epoch 101\n","togrep : results : epoch 101 ; accuracy test : 14.08, precision@10 test : 70.05, reward_accuracy test 57.77\n","The original reward is:1.8911\n","The optimal reward is:1.6269\n","\n","The original reward is: 1.8502\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eooo4GMdpGQb","executionInfo":{"status":"ok","timestamp":1620557536433,"user_tz":240,"elapsed":327,"user":{"displayName":"Aathira Manoj","photoUrl":"","userId":"16371146117290387708"}},"outputId":"677162c4-7524-4b15-c570-cf1316445bb9"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["action_gen_real.txt  data.py\t       model_output\t reward_gen_real.txt\n","action_gen.txt\t     discriminator.py  nn_layer.py\t reward_gen.txt\n","agent.py\t     eval.py\t       pg_accuracy6.png  tar_gen_real.txt\n","click_gen_real.txt   generator.py      pg_map6.png\t tar_gen.txt\n","click_gen.txt\t     helper.py\t       __pycache__\t train.py\n","data.png\t     main.py\t       replay.py\t util.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hoM2vSdi5zEf","executionInfo":{"status":"ok","timestamp":1620560003733,"user_tz":240,"elapsed":715,"user":{"displayName":"Aathira Manoj","photoUrl":"","userId":"16371146117290387708"}},"outputId":"5e6b04f4-e469-4b1a-cc83-7273ccbbd072"},"source":["!pwd"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/NYU_Spring_2021/DLS/rl_rec/Model-Based-Reinforcement-Learning-for-Online-Recommendation-master/IRecGAN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-CjzoSH_wZsi","executionInfo":{"status":"ok","timestamp":1620560755324,"user_tz":240,"elapsed":757,"user":{"displayName":"Aathira Manoj","photoUrl":"","userId":"16371146117290387708"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4zoVbnS_544I"},"source":[""],"execution_count":null,"outputs":[]}]}